{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o-sXSz3pnqC"
      },
      "source": [
        "# Optimized Image Generator\n",
        "\n",
        "This notebook fetches configuration from the prompt-generator API and generates images using SDXL.\n",
        "\n",
        "**Features:**\n",
        "- Configuration pulled from web UI (no manual parameter editing)\n",
        "- Three prompt modes: Full, Refined, Dual-Encoder\n",
        "- Compel integration for long prompts\n",
        "- Memory optimized for Colab free tier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5LLkj8qpnqI"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Setup & Configuration Fetch\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import drive\n",
        "import requests\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore', message='Flax classes are deprecated')\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# Mount Google Drive\n",
        "mount_path = '/content/drive'\n",
        "if not os.path.exists(mount_path):\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount(mount_path)\n",
        "else:\n",
        "    print(\"Drive already mounted.\")\n",
        "\n",
        "# Load HuggingFace token\n",
        "env_path = '/content/drive/MyDrive/AI/hf_token.env'\n",
        "load_dotenv(env_path)\n",
        "huggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n",
        "\n",
        "# API Configuration\n",
        "API_BASE = \"https://prompt-gen.squigglypickle.co.uk\"\n",
        "\n",
        "# Fetch configuration from API\n",
        "print(\"\\nFetching configuration from API...\")\n",
        "try:\n",
        "    config_response = requests.get(f\"{API_BASE}/image-config\", timeout=10)\n",
        "    config_response.raise_for_status()\n",
        "    config = config_response.json()['config']\n",
        "    print(\"✓ Configuration loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to fetch config: {e}\")\n",
        "    print(\"Using default configuration...\")\n",
        "    config = {\n",
        "        'prompt_mode': 'refined',\n",
        "        'base_prompt': '',\n",
        "        'sfw': False,\n",
        "        'selected_categories': [],\n",
        "        'model_id': 'John6666/wai-ani-nsfw-ponyxl-v11-sdxl',\n",
        "        'download_model': False,\n",
        "        'schedulers': ['DPM++ 2M', 'Euler'],\n",
        "        'guidance_low': 5,\n",
        "        'guidance_high': 15,\n",
        "        'steps_low': 30,\n",
        "        'steps_high': 40,\n",
        "        'resolution_mode': 'sdxl',\n",
        "        'num_images': 25,\n",
        "        'prompts_per_batch': 3,\n",
        "    }\n",
        "\n",
        "# LoRA Configuration (set these manually — not fetched from API)\n",
        "lora_enabled = True #@param {type:\"boolean\"}\n",
        "lora_path = \"Loras/stoo_tee/output/stoo_tee.safetensors\" #@param {type:\"string\"}\n",
        "lora_strength_start = 0.5 #@param {type:\"slider\", min:0.1, max:1.5, step:0.1}\n",
        "lora_strength_end = 1.0 #@param {type:\"slider\", min:0.1, max:1.5, step:0.1}\n",
        "lora_strength_iterations = 5 #@param {type:\"integer\", min:1, max:10}\n",
        "lora_trigger_word = \"stoo_tee\" #@param {type:\"string\"}\n",
        "lora_prepend_trigger = True #@param {type:\"boolean\"}\n",
        "\n",
        "config['lora_enabled'] = lora_enabled\n",
        "config['lora_path'] = f\"/content/drive/MyDrive/{lora_path}\" if lora_enabled else None\n",
        "config['lora_strength_start'] = lora_strength_start\n",
        "config['lora_strength_end'] = lora_strength_end\n",
        "config['lora_strength_iterations'] = lora_strength_iterations\n",
        "config['lora_trigger_word'] = lora_trigger_word\n",
        "config['lora_prepend_trigger'] = lora_prepend_trigger\n",
        "\n",
        "# Generate LoRA strength values\n",
        "if config['lora_enabled']:\n",
        "    config['lora_strengths'] = np.linspace(\n",
        "        config['lora_strength_start'],\n",
        "        config['lora_strength_end'],\n",
        "        config['lora_strength_iterations']\n",
        "    ).tolist()\n",
        "else:\n",
        "    config['lora_strengths'] = []\n",
        "\n",
        "# Display configuration\n",
        "print(f\"\\n=== Configuration ===\")\n",
        "print(f\"  Prompt Mode: {config['prompt_mode']}\")\n",
        "print(f\"  Model: {config['model_id']}\")\n",
        "print(f\"  Schedulers: {', '.join(config['schedulers'])}\")\n",
        "num_lora_strengths = len(config['lora_strengths']) if config['lora_enabled'] else 1\n",
        "print(f\"  Images: {config['num_images']} x {len(config['schedulers'])} x {num_lora_strengths} = {config['num_images'] * len(config['schedulers']) * num_lora_strengths}\")\n",
        "print(f\"  Guidance: {config['guidance_low']}-{config['guidance_high']}\")\n",
        "print(f\"  Steps: {config['steps_low']}-{config['steps_high']}\")\n",
        "print(f\"  Resolution: {config['resolution_mode']}\")\n",
        "print(f\"  SFW: {config['sfw']}\")\n",
        "if lora_enabled:\n",
        "    print(f\"  LoRA: {lora_path}\")\n",
        "    print(f\"    Strength range: {lora_strength_start} → {lora_strength_end} ({lora_strength_iterations} steps)\")\n",
        "    print(f\"    Values: {[round(s, 2) for s in config['lora_strengths']]}\")\n",
        "    print(f\"    Trigger: '{lora_trigger_word}' (auto-prepend={'on' if lora_prepend_trigger else 'off'})\")\n",
        "else:\n",
        "    print(f\"  LoRA: disabled\")\n",
        "\n",
        "# Derived paths\n",
        "from datetime import datetime\n",
        "base_path = \"/content/drive/MyDrive/AI/\"\n",
        "model_path = base_path + \"models/\" + config['model_id']\n",
        "save_directory = f\"{base_path}images/{datetime.now().strftime('%Y%m%d%H%M%S')}/\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "print(f\"\\nSave directory: {save_directory}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyih2UFHpnqK",
        "outputId": "f4a6169c-b805-4356-e0d0-a89f888096d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.9.0+cu126\n",
            "CUDA: True\n",
            "GPU: Tesla T4\n",
            "Memory: 14.64GB free\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Install & Import Dependencies\n",
        "!pip install -q diffusers transformers accelerate safetensors compel\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import json\n",
        "import gc\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from huggingface_hub import snapshot_download\n",
        "from diffusers import (\n",
        "    StableDiffusionXLPipeline,\n",
        "    AutoencoderKL,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DPMSolverSinglestepScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    KDPM2DiscreteScheduler,\n",
        "    KDPM2AncestralDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    DDIMScheduler,\n",
        "    PNDMScheduler,\n",
        "    UniPCMultistepScheduler\n",
        ")\n",
        "from compel import CompelForSDXL\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6PKhIZ4pnqM"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Load Model & Pipeline\n",
        "model_id = config['model_id']\n",
        "model_exists = os.path.exists(model_path) and os.path.isdir(model_path) and len(os.listdir(model_path)) > 0\n",
        "\n",
        "if config['download_model'] or not model_exists:\n",
        "    print(f\"Downloading model {model_id}...\")\n",
        "    snapshot_download(\n",
        "        repo_id=model_id,\n",
        "        local_dir=model_path,\n",
        "        token=huggingface_token if huggingface_token else None,\n",
        "        ignore_patterns=[\"*.safetensors.lock\"]\n",
        "    )\n",
        "    print(f\"✓ Model downloaded\")\n",
        "else:\n",
        "    print(f\"✓ Model exists at {model_path}\")\n",
        "\n",
        "# Load pipeline\n",
        "print(f\"\\nLoading pipeline...\")\n",
        "load_kwargs = {\n",
        "    \"torch_dtype\": torch.float16,\n",
        "    \"use_safetensors\": True,\n",
        "}\n",
        "\n",
        "pipe = None\n",
        "for attempt in [\"fp16\", \"no_variant\"]:\n",
        "    try:\n",
        "        if attempt == \"fp16\":\n",
        "            load_kwargs[\"variant\"] = \"fp16\"\n",
        "        else:\n",
        "            load_kwargs.pop(\"variant\", None)\n",
        "\n",
        "        try:\n",
        "            pipe = StableDiffusionXLPipeline.from_pretrained(model_path, **load_kwargs)\n",
        "            print(f\"✓ Loaded from local path\")\n",
        "        except (OSError, FileNotFoundError):\n",
        "            load_kwargs[\"token\"] = huggingface_token\n",
        "            pipe = StableDiffusionXLPipeline.from_pretrained(model_id, **load_kwargs)\n",
        "            print(f\"✓ Loaded from HuggingFace\")\n",
        "        break\n",
        "    except ValueError as e:\n",
        "        if \"variant\" in str(e) and attempt == \"fp16\":\n",
        "            continue\n",
        "        raise\n",
        "\n",
        "# Load enhanced VAE\n",
        "print(\"Loading enhanced VAE...\")\n",
        "try:\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        \"madebyollin/sdxl-vae-fp16-fix\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    pipe.vae = vae\n",
        "    print(\"✓ Enhanced VAE applied\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Could not load enhanced VAE: {e}\")\n",
        "\n",
        "# Move to GPU and optimize\n",
        "pipe = pipe.to(\"cuda\")\n",
        "pipe.enable_attention_slicing()\n",
        "pipe.enable_vae_slicing()\n",
        "pipe.enable_vae_tiling()\n",
        "\n",
        "try:\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "    print(\"✓ xformers enabled\")\n",
        "except:\n",
        "    print(\"⚠ xformers not available\")\n",
        "\n",
        "# Load LoRA if enabled (kept unfused — strength applied via cross_attention_kwargs at inference)\n",
        "if config['lora_enabled'] and config['lora_path']:\n",
        "    lora_file = config['lora_path']\n",
        "    if os.path.exists(lora_file):\n",
        "        print(f\"Loading LoRA from {lora_file}...\")\n",
        "        pipe.load_lora_weights(lora_file)\n",
        "        print(f\"✓ LoRA loaded (unfused — strength set dynamically via cross_attention_kwargs)\")\n",
        "        print(f\"  Strengths to use: {[round(s, 2) for s in config['lora_strengths']]}\")\n",
        "        print(f\"  Trigger word: '{config['lora_trigger_word']}' — include this in your prompts\")\n",
        "    else:\n",
        "        print(f\"✗ LoRA file not found: {lora_file}\")\n",
        "        config['lora_enabled'] = False\n",
        "\n",
        "# Initialize Compel for prompt weighting\n",
        "compel = CompelForSDXL(pipe)\n",
        "print(\"✓ Compel initialized\")\n",
        "\n",
        "print(f\"\\n✓ Pipeline ready! GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1xThR82pnqO",
        "outputId": "36133089-a09c-42ef-86e8-e99efc4398d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ 2 schedulers configured: Euler, DPM++ 2M\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Initialize Schedulers\n",
        "def initialize_samplers(base_config):\n",
        "    \"\"\"Create scheduler instances from model's config.\"\"\"\n",
        "    clean_config = {k: v for k, v in base_config.items() if k not in ['beta_schedule']}\n",
        "\n",
        "    return {\n",
        "        \"Model Default\": lambda: pipe.scheduler.__class__.from_config(base_config),\n",
        "        \"DPM++ 2M\": lambda: DPMSolverMultistepScheduler.from_config(base_config),\n",
        "        \"DPM++ 2M Karras\": lambda: DPMSolverMultistepScheduler.from_config(clean_config, use_karras_sigmas=True),\n",
        "        \"DPM++ SDE\": lambda: DPMSolverSinglestepScheduler.from_config(base_config),\n",
        "        \"DPM++ SDE Karras\": lambda: DPMSolverSinglestepScheduler.from_config(clean_config, use_karras_sigmas=True),\n",
        "        \"Euler\": lambda: EulerDiscreteScheduler.from_config(base_config),\n",
        "        \"Euler a\": lambda: EulerAncestralDiscreteScheduler.from_config(base_config),\n",
        "        \"Heun\": lambda: HeunDiscreteScheduler.from_config(base_config),\n",
        "        \"KDPM2\": lambda: KDPM2DiscreteScheduler.from_config(base_config),\n",
        "        \"KDPM2 a\": lambda: KDPM2AncestralDiscreteScheduler.from_config(base_config),\n",
        "        \"LMS\": lambda: LMSDiscreteScheduler.from_config(base_config),\n",
        "        \"DDIM\": lambda: DDIMScheduler.from_config(base_config),\n",
        "        \"PNDM\": lambda: PNDMScheduler.from_config(base_config),\n",
        "        \"UniPC\": lambda: UniPCMultistepScheduler.from_config(base_config),\n",
        "    }\n",
        "\n",
        "SAMPLERS = initialize_samplers(pipe.scheduler.config)\n",
        "\n",
        "# Validate selected schedulers\n",
        "selected_schedulers = [s for s in config['schedulers'] if s in SAMPLERS]\n",
        "if not selected_schedulers:\n",
        "    selected_schedulers = [\"Euler\"]\n",
        "    print(\"\\u26a0 No valid schedulers found, defaulting to Euler\")\n",
        "\n",
        "print(f\"\\u2713 {len(selected_schedulers)} schedulers configured: {', '.join(selected_schedulers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKmC2V2IpnqP",
        "outputId": "af43103b-a8d0-44ec-dd9a-6357ce3e4ac4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-fetching 3 prompts...\n",
            "✓ Fetched 3 prompts\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Prompt Manager\n",
        "NEGATIVE_PROMPT = \"text, writing, bad teeth, deformed face, child, childish, young, deformed, extra fingers\"\n",
        "\n",
        "# Resolution pools\n",
        "CLASSIC_RESOLUTIONS = [720, 768, 800, 1024]\n",
        "SDXL_RESOLUTIONS = [\n",
        "    (1024, 1024), (1152, 896), (896, 1152), (1216, 832),\n",
        "    (832, 1216), (1344, 768), (768, 1344)\n",
        "]\n",
        "\n",
        "class PromptManager:\n",
        "    def __init__(self, api_base, config):\n",
        "        self.api_base = api_base\n",
        "        self.config = config\n",
        "        self.cache = []\n",
        "        self.stats = defaultdict(int)\n",
        "\n",
        "    def fetch_prompts(self, count=3):\n",
        "        \"\"\"Fetch prompts from API.\"\"\"\n",
        "        prompts = []\n",
        "        for _ in range(count):\n",
        "            try:\n",
        "                params = {\n",
        "                    'sfw': self.config['sfw'],\n",
        "                    'encoder-split': 'true',  # Always get dual-encoder data\n",
        "                }\n",
        "                if self.config['base_prompt']:\n",
        "                    params['base_prompt'] = self.config['base_prompt']\n",
        "                if self.config['selected_categories']:\n",
        "                    params['selected_categories'] = ' '.join(self.config['selected_categories'])\n",
        "\n",
        "                response = requests.get(f\"{self.api_base}/generate-refined\", params=params, timeout=300)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    prompts.append({\n",
        "                        'full': data.get('full_prompt', ''),\n",
        "                        'refined': data.get('refined_prompt', ''),\n",
        "                        'openclip_g': data.get('openclip_g', ''),\n",
        "                        'clip_l': data.get('clip_l', ''),\n",
        "                        'negative': data.get('negative_prompt', NEGATIVE_PROMPT)\n",
        "                    })\n",
        "                    self.stats['fetched'] += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching prompt: {e}\")\n",
        "                self.stats['failed'] += 1\n",
        "\n",
        "        self.cache.extend(prompts)\n",
        "        return len(prompts)\n",
        "\n",
        "    def get_prompt(self):\n",
        "        \"\"\"Get a prompt from cache.\"\"\"\n",
        "        if len(self.cache) < 3:\n",
        "            print(f\"Cache low ({len(self.cache)}), fetching more...\")\n",
        "            self.fetch_prompts(self.config['prompts_per_batch'])\n",
        "\n",
        "        if self.cache:\n",
        "            self.stats['used'] += 1\n",
        "            return self.cache.pop(0)\n",
        "        return None\n",
        "\n",
        "    def get_final_prompt(self, prompt_data):\n",
        "        \"\"\"Get the appropriate prompt based on prompt_mode.\"\"\"\n",
        "        mode = self.config['prompt_mode']\n",
        "\n",
        "        if mode == 'dual_encoder':\n",
        "            return {\n",
        "                'type': 'dual',\n",
        "                'prompt': prompt_data.get('openclip_g') or prompt_data.get('refined') or prompt_data['full'],\n",
        "                'prompt_2': prompt_data.get('clip_l') or prompt_data.get('refined') or prompt_data['full'],\n",
        "                'negative': prompt_data['negative']\n",
        "            }\n",
        "        elif mode == 'refined' and prompt_data.get('refined'):\n",
        "            return {\n",
        "                'type': 'single',\n",
        "                'prompt': prompt_data['refined'],\n",
        "                'negative': prompt_data['negative']\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'type': 'single',\n",
        "                'prompt': prompt_data['full'],\n",
        "                'negative': prompt_data['negative']\n",
        "            }\n",
        "\n",
        "def get_resolution():\n",
        "    \"\"\"Get random resolution based on config.\"\"\"\n",
        "    if config['resolution_mode'] == 'classic':\n",
        "        return (random.choice(CLASSIC_RESOLUTIONS), random.choice(CLASSIC_RESOLUTIONS))\n",
        "    return random.choice(SDXL_RESOLUTIONS)\n",
        "\n",
        "# Initialize\n",
        "prompt_manager = PromptManager(API_BASE, config)\n",
        "print(f\"Pre-fetching {config['prompts_per_batch']} prompts...\")\n",
        "fetched = prompt_manager.fetch_prompts(config['prompts_per_batch'])\n",
        "print(f\"\\u2713 Fetched {fetched} prompts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Elbr4i6ppnqR"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Generation Loop\n",
        "metadata_list = []\n",
        "metadata_path = os.path.join(save_directory, \"metadata.json\")\n",
        "\n",
        "num_images = config['num_images']\n",
        "num_schedulers = len(selected_schedulers)\n",
        "num_lora_strengths = len(config['lora_strengths']) if config['lora_enabled'] else 1\n",
        "total_images = num_images * num_schedulers * num_lora_strengths\n",
        "\n",
        "print(f\"Starting generation: {num_images} prompts x {num_schedulers} schedulers x {num_lora_strengths} LoRA strengths = {total_images} images\")\n",
        "if config['lora_enabled']:\n",
        "    if config['lora_prepend_trigger']:\n",
        "        print(f\"LoRA active: trigger word '{config['lora_trigger_word']}' will be prepended to prompts\")\n",
        "    else:\n",
        "        print(f\"LoRA active: auto-prepend OFF — include '{config['lora_trigger_word']}' in your prompts manually\")\n",
        "print(f\"Save directory: {save_directory}\\n\")\n",
        "\n",
        "image_counter = 0\n",
        "for i in range(num_images):\n",
        "    try:\n",
        "        # Get prompt\n",
        "        prompt_data = prompt_manager.get_prompt()\n",
        "        if not prompt_data:\n",
        "            print(f\"[{i+1}/{num_images}] Failed to get prompt, skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Get final prompt based on mode\n",
        "        final = prompt_manager.get_final_prompt(prompt_data)\n",
        "\n",
        "        # Prepend LoRA trigger word if enabled and auto-prepend is on\n",
        "        if config['lora_enabled'] and config['lora_prepend_trigger'] and config['lora_trigger_word']:\n",
        "            trigger = config['lora_trigger_word']\n",
        "            final['prompt'] = f\"{trigger}, {final['prompt']}\"\n",
        "            if 'prompt_2' in final and final['prompt_2']:\n",
        "                final['prompt_2'] = f\"{trigger}, {final['prompt_2']}\"\n",
        "\n",
        "        # Random parameters (same for all schedulers and LoRA strengths)\n",
        "        width, height = get_resolution()\n",
        "        guidance_scale = round(random.uniform(config['guidance_low'], config['guidance_high']) * 2) / 2\n",
        "        num_steps = random.randint(config['steps_low'], config['steps_high'])\n",
        "        seed = random.randint(0, 2**32 - 1)\n",
        "\n",
        "        # Generate with each scheduler\n",
        "        for scheduler_name in selected_schedulers:\n",
        "            try:\n",
        "                pipe.scheduler = SAMPLERS[scheduler_name]()\n",
        "\n",
        "                # Generate with each LoRA strength\n",
        "                lora_strengths = config['lora_strengths'] if config['lora_enabled'] else [0]\n",
        "                for lora_strength in lora_strengths:\n",
        "                    try:\n",
        "                        # Reset generator to same seed for each LoRA strength\n",
        "                        # so the only variable between images is the LoRA strength\n",
        "                        generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "                        # Set LoRA scale via cross_attention_kwargs (no fuse/unfuse needed)\n",
        "                        lora_kwargs = {}\n",
        "                        if config['lora_enabled'] and config['lora_path']:\n",
        "                            lora_kwargs[\"cross_attention_kwargs\"] = {\"scale\": lora_strength}\n",
        "\n",
        "                        # Generate based on prompt type\n",
        "                        if final['type'] == 'dual':\n",
        "                            # Dual-encoder mode: pass separate prompts\n",
        "                            result = pipe(\n",
        "                                prompt=final['prompt'],\n",
        "                                prompt_2=final['prompt_2'],\n",
        "                                negative_prompt=final['negative'],\n",
        "                                width=width,\n",
        "                                height=height,\n",
        "                                guidance_scale=guidance_scale,\n",
        "                                num_inference_steps=num_steps,\n",
        "                                generator=generator,\n",
        "                                **lora_kwargs,\n",
        "                            ).images[0]\n",
        "                        else:\n",
        "                            # Single prompt mode: use Compel for weighting\n",
        "                            conditioning = compel(final['prompt'], negative_prompt=final['negative'])\n",
        "                            result = pipe(\n",
        "                                prompt_embeds=conditioning.embeds,\n",
        "                                pooled_prompt_embeds=conditioning.pooled_embeds,\n",
        "                                negative_prompt_embeds=conditioning.negative_embeds,\n",
        "                                negative_pooled_prompt_embeds=conditioning.negative_pooled_embeds,\n",
        "                                width=width,\n",
        "                                height=height,\n",
        "                                guidance_scale=guidance_scale,\n",
        "                                num_inference_steps=num_steps,\n",
        "                                generator=generator,\n",
        "                                **lora_kwargs,\n",
        "                            ).images[0]\n",
        "\n",
        "                        # Save image with LoRA strength in filename\n",
        "                        scheduler_short = scheduler_name.replace(' ', '_').replace('+', 'p')\n",
        "                        lora_suffix = f\"_lora{lora_strength:.2f}\" if config['lora_enabled'] else \"\"\n",
        "                        filename = f\"{datetime.now().strftime('%Y%m%d%H%M%S')}_{str(image_counter).zfill(4)}_{scheduler_short}{lora_suffix}.png\"\n",
        "                        result.save(os.path.join(save_directory, filename))\n",
        "\n",
        "                        # Save metadata\n",
        "                        metadata = {\n",
        "                            \"filename\": filename,\n",
        "                            \"model\": config['model_id'],\n",
        "                            \"scheduler\": scheduler_name,\n",
        "                            \"prompt_mode\": config['prompt_mode'],\n",
        "                            \"prompt\": final['prompt'],\n",
        "                            \"prompt_2\": final.get('prompt_2', ''),\n",
        "                            \"negative_prompt\": final['negative'],\n",
        "                            \"sfw\": config['sfw'],\n",
        "                            \"seed\": seed,\n",
        "                            \"width\": width,\n",
        "                            \"height\": height,\n",
        "                            \"guidance_scale\": guidance_scale,\n",
        "                            \"num_steps\": num_steps,\n",
        "                            \"prompt_index\": i,\n",
        "                            \"lora_enabled\": config['lora_enabled'],\n",
        "                            \"lora_path\": config.get('lora_path', ''),\n",
        "                            \"lora_strength\": lora_strength if config['lora_enabled'] else 0,\n",
        "                            \"lora_trigger_word\": config.get('lora_trigger_word', ''),\n",
        "                        }\n",
        "                        metadata_list.append(metadata)\n",
        "\n",
        "                        with open(metadata_path, 'w') as f:\n",
        "                            json.dump(metadata_list, f, indent=2)\n",
        "\n",
        "                        del result\n",
        "                        torch.cuda.empty_cache()\n",
        "                        image_counter += 1\n",
        "\n",
        "                        if config['lora_enabled']:\n",
        "                            print(f\"  [{image_counter}/{total_images}] LoRA={lora_strength:.2f} | {scheduler_name}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error generating with LoRA strength {lora_strength}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error with scheduler {scheduler_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Progress\n",
        "        print(f\"[{i+1}/{num_images}] seed={seed} | {width}x{height} | {num_steps} steps | G:{guidance_scale:.1f} | {num_schedulers} schedulers x {len(lora_strengths)} strengths\")\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error on image {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n✓ Generation complete!\")\n",
        "print(f\"✓ {len(metadata_list)} images saved to: {save_directory}\")\n",
        "print(f\"✓ Metadata saved to: {metadata_path}\")\n",
        "print(f\"\\nPrompt stats: Fetched={prompt_manager.stats['fetched']}, Used={prompt_manager.stats['used']}, Failed={prompt_manager.stats['failed']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJhJgvAkpnqS"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Display Results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_columns = 3\n",
        "max_images = 12\n",
        "\n",
        "if metadata_list:\n",
        "    display_meta = metadata_list[:max_images]\n",
        "    num_rows = (len(display_meta) + num_columns - 1) // num_columns\n",
        "\n",
        "    plt.figure(figsize=(5 * num_columns, 5 * num_rows))\n",
        "\n",
        "    for idx, meta in enumerate(display_meta):\n",
        "        img = Image.open(os.path.join(save_directory, meta['filename']))\n",
        "        plt.subplot(num_rows, num_columns, idx + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{meta['scheduler']}\\n{meta['width']}x{meta['height']}\", fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"Displayed {len(display_meta)} of {len(metadata_list)} images\")\n",
        "else:\n",
        "    print(\"No images to display\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzilkGeIpnqU",
        "outputId": "e9661235-18a0-4393-8a5b-8751c671dea6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Cleanup complete\n",
            "GPU Memory: 7.45GB free\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Cleanup\n",
        "del pipe, compel\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\u2713 Cleanup complete\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}