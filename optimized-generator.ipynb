{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Image Generator\n",
    "\n",
    "This notebook fetches configuration from the prompt-generator API and generates images using SDXL.\n",
    "\n",
    "**Features:**\n",
    "- Configuration pulled from web UI (no manual parameter editing)\n",
    "- Three prompt modes: Full, Refined, Dual-Encoder\n",
    "- Compel integration for long prompts\n",
    "- Memory optimized for Colab free tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Configuration Fetch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google.colab import drive\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', message='Flax classes are deprecated')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Mount Google Drive\n",
    "mount_path = '/content/drive'\n",
    "if not os.path.exists(mount_path):\n",
    "    print(\"Mounting Google Drive...\")\n",
    "    drive.mount(mount_path)\n",
    "else:\n",
    "    print(\"Drive already mounted.\")\n",
    "\n",
    "# Load HuggingFace token\n",
    "env_path = '/content/drive/MyDrive/AI/hf_token.env'\n",
    "load_dotenv(env_path)\n",
    "huggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "# API Configuration\n",
    "API_BASE = \"https://prompt-gen.squigglypickle.co.uk\"\n",
    "\n",
    "# Fetch configuration from API\n",
    "print(\"\\nFetching configuration from API...\")\n",
    "try:\n",
    "    config_response = requests.get(f\"{API_BASE}/image-config\", timeout=10)\n",
    "    config_response.raise_for_status()\n",
    "    config = config_response.json()['config']\n",
    "    print(\"\\u2713 Configuration loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\u2717 Failed to fetch config: {e}\")\n",
    "    print(\"Using default configuration...\")\n",
    "    config = {\n",
    "        'prompt_mode': 'refined',\n",
    "        'base_prompt': '',\n",
    "        'sfw': False,\n",
    "        'selected_categories': [],\n",
    "        'model_id': 'John6666/wai-ani-nsfw-ponyxl-v11-sdxl',\n",
    "        'download_model': False,\n",
    "        'schedulers': ['DPM++ 2M', 'Euler'],\n",
    "        'guidance_low': 5,\n",
    "        'guidance_high': 15,\n",
    "        'steps_low': 30,\n",
    "        'steps_high': 40,\n",
    "        'resolution_mode': 'sdxl',\n",
    "        'num_images': 25,\n",
    "        'prompts_per_batch': 3,\n",
    "    }\n",
    "\n",
    "# Display configuration\n",
    "print(f\"\\n=== Configuration ===\")\n",
    "print(f\"  Prompt Mode: {config['prompt_mode']}\")\n",
    "print(f\"  Model: {config['model_id']}\")\n",
    "print(f\"  Schedulers: {', '.join(config['schedulers'])}\")\n",
    "print(f\"  Images: {config['num_images']} x {len(config['schedulers'])} = {config['num_images'] * len(config['schedulers'])}\")\n",
    "print(f\"  Guidance: {config['guidance_low']}-{config['guidance_high']}\")\n",
    "print(f\"  Steps: {config['steps_low']}-{config['steps_high']}\")\n",
    "print(f\"  Resolution: {config['resolution_mode']}\")\n",
    "print(f\"  SFW: {config['sfw']}\")\n",
    "\n",
    "# Derived paths\n",
    "from datetime import datetime\n",
    "base_path = \"/content/drive/MyDrive/AI/\"\n",
    "model_path = base_path + \"models/\" + config['model_id']\n",
    "save_directory = f\"{base_path}images/{datetime.now().strftime('%Y%m%d%H%M%S')}/\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "print(f\"\\nSave directory: {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install & Import Dependencies\n",
    "!pip install -q diffusers transformers accelerate safetensors compel\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from huggingface_hub import snapshot_download\n",
    "from diffusers import (\n",
    "    StableDiffusionXLPipeline,\n",
    "    AutoencoderKL,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    DPMSolverSinglestepScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    "    KDPM2DiscreteScheduler,\n",
    "    KDPM2AncestralDiscreteScheduler,\n",
    "    HeunDiscreteScheduler,\n",
    "    LMSDiscreteScheduler,\n",
    "    DDIMScheduler,\n",
    "    PNDMScheduler,\n",
    "    UniPCMultistepScheduler\n",
    ")\n",
    "from compel import CompelForSDXL\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Model & Pipeline\n",
    "model_id = config['model_id']\n",
    "model_exists = os.path.exists(model_path) and os.path.isdir(model_path) and len(os.listdir(model_path)) > 0\n",
    "\n",
    "if config['download_model'] or not model_exists:\n",
    "    print(f\"Downloading model {model_id}...\")\n",
    "    snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        local_dir=model_path,\n",
    "        token=huggingface_token if huggingface_token else None,\n",
    "        ignore_patterns=[\"*.safetensors.lock\"]\n",
    "    )\n",
    "    print(f\"\\u2713 Model downloaded\")\n",
    "else:\n",
    "    print(f\"\\u2713 Model exists at {model_path}\")\n",
    "\n",
    "# Load pipeline\n",
    "print(f\"\\nLoading pipeline...\")\n",
    "load_kwargs = {\n",
    "    \"torch_dtype\": torch.float16,\n",
    "    \"use_safetensors\": True,\n",
    "}\n",
    "\n",
    "pipe = None\n",
    "for attempt in [\"fp16\", \"no_variant\"]:\n",
    "    try:\n",
    "        if attempt == \"fp16\":\n",
    "            load_kwargs[\"variant\"] = \"fp16\"\n",
    "        else:\n",
    "            load_kwargs.pop(\"variant\", None)\n",
    "        \n",
    "        try:\n",
    "            pipe = StableDiffusionXLPipeline.from_pretrained(model_path, **load_kwargs)\n",
    "            print(f\"\\u2713 Loaded from local path\")\n",
    "        except (OSError, FileNotFoundError):\n",
    "            load_kwargs[\"token\"] = huggingface_token\n",
    "            pipe = StableDiffusionXLPipeline.from_pretrained(model_id, **load_kwargs)\n",
    "            print(f\"\\u2713 Loaded from HuggingFace\")\n",
    "        break\n",
    "    except ValueError as e:\n",
    "        if \"variant\" in str(e) and attempt == \"fp16\":\n",
    "            continue\n",
    "        raise\n",
    "\n",
    "# Load enhanced VAE\n",
    "print(\"Loading enhanced VAE...\")\n",
    "try:\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        \"madebyollin/sdxl-vae-fp16-fix\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    pipe.vae = vae\n",
    "    print(\"\\u2713 Enhanced VAE applied\")\n",
    "except Exception as e:\n",
    "    print(f\"\\u26a0 Could not load enhanced VAE: {e}\")\n",
    "\n",
    "# Move to GPU and optimize\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.enable_attention_slicing()\n",
    "pipe.enable_vae_slicing()\n",
    "pipe.enable_vae_tiling()\n",
    "\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    print(\"\\u2713 xformers enabled\")\n",
    "except:\n",
    "    print(\"\\u26a0 xformers not available\")\n",
    "\n",
    "# Initialize Compel for prompt weighting\n",
    "compel = CompelForSDXL(pipe)\n",
    "print(\"\\u2713 Compel initialized\")\n",
    "\n",
    "print(f\"\\n\\u2713 Pipeline ready! GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Initialize Schedulers\n",
    "def initialize_samplers(base_config):\n",
    "    \"\"\"Create scheduler instances from model's config.\"\"\"\n",
    "    clean_config = {k: v for k, v in base_config.items() if k not in ['beta_schedule']}\n",
    "    \n",
    "    return {\n",
    "        \"Model Default\": lambda: pipe.scheduler.__class__.from_config(base_config),\n",
    "        \"DPM++ 2M\": lambda: DPMSolverMultistepScheduler.from_config(base_config),\n",
    "        \"DPM++ 2M Karras\": lambda: DPMSolverMultistepScheduler.from_config(clean_config, use_karras_sigmas=True),\n",
    "        \"DPM++ SDE\": lambda: DPMSolverSinglestepScheduler.from_config(base_config),\n",
    "        \"DPM++ SDE Karras\": lambda: DPMSolverSinglestepScheduler.from_config(clean_config, use_karras_sigmas=True),\n",
    "        \"Euler\": lambda: EulerDiscreteScheduler.from_config(base_config),\n",
    "        \"Euler a\": lambda: EulerAncestralDiscreteScheduler.from_config(base_config),\n",
    "        \"Heun\": lambda: HeunDiscreteScheduler.from_config(base_config),\n",
    "        \"KDPM2\": lambda: KDPM2DiscreteScheduler.from_config(base_config),\n",
    "        \"KDPM2 a\": lambda: KDPM2AncestralDiscreteScheduler.from_config(base_config),\n",
    "        \"LMS\": lambda: LMSDiscreteScheduler.from_config(base_config),\n",
    "        \"DDIM\": lambda: DDIMScheduler.from_config(base_config),\n",
    "        \"PNDM\": lambda: PNDMScheduler.from_config(base_config),\n",
    "        \"UniPC\": lambda: UniPCMultistepScheduler.from_config(base_config),\n",
    "    }\n",
    "\n",
    "SAMPLERS = initialize_samplers(pipe.scheduler.config)\n",
    "\n",
    "# Validate selected schedulers\n",
    "selected_schedulers = [s for s in config['schedulers'] if s in SAMPLERS]\n",
    "if not selected_schedulers:\n",
    "    selected_schedulers = [\"Euler\"]\n",
    "    print(\"\\u26a0 No valid schedulers found, defaulting to Euler\")\n",
    "\n",
    "print(f\"\\u2713 {len(selected_schedulers)} schedulers configured: {', '.join(selected_schedulers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Prompt Manager\n",
    "NEGATIVE_PROMPT = \"text, writing, bad teeth, deformed face, child, childish, young, deformed, extra fingers\"\n",
    "\n",
    "# Resolution pools\n",
    "CLASSIC_RESOLUTIONS = [720, 768, 800, 1024]\n",
    "SDXL_RESOLUTIONS = [\n",
    "    (1024, 1024), (1152, 896), (896, 1152), (1216, 832),\n",
    "    (832, 1216), (1344, 768), (768, 1344)\n",
    "]\n",
    "\n",
    "class PromptManager:\n",
    "    def __init__(self, api_base, config):\n",
    "        self.api_base = api_base\n",
    "        self.config = config\n",
    "        self.cache = []\n",
    "        self.stats = defaultdict(int)\n",
    "    \n",
    "    def fetch_prompts(self, count=3):\n",
    "        \"\"\"Fetch prompts from API.\"\"\"\n",
    "        prompts = []\n",
    "        for _ in range(count):\n",
    "            try:\n",
    "                params = {\n",
    "                    'sfw': self.config['sfw'],\n",
    "                    'encoder-split': 'true',  # Always get dual-encoder data\n",
    "                }\n",
    "                if self.config['base_prompt']:\n",
    "                    params['base_prompt'] = self.config['base_prompt']\n",
    "                if self.config['selected_categories']:\n",
    "                    params['selected_categories'] = ' '.join(self.config['selected_categories'])\n",
    "                \n",
    "                response = requests.get(f\"{self.api_base}/generate-refined\", params=params, timeout=30)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    prompts.append({\n",
    "                        'full': data.get('full_prompt', ''),\n",
    "                        'refined': data.get('refined_prompt', ''),\n",
    "                        'openclip_g': data.get('openclip_g', ''),\n",
    "                        'clip_l': data.get('clip_l', ''),\n",
    "                        'negative': data.get('negative_prompt', NEGATIVE_PROMPT)\n",
    "                    })\n",
    "                    self.stats['fetched'] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching prompt: {e}\")\n",
    "                self.stats['failed'] += 1\n",
    "        \n",
    "        self.cache.extend(prompts)\n",
    "        return len(prompts)\n",
    "    \n",
    "    def get_prompt(self):\n",
    "        \"\"\"Get a prompt from cache.\"\"\"\n",
    "        if len(self.cache) < 3:\n",
    "            print(f\"Cache low ({len(self.cache)}), fetching more...\")\n",
    "            self.fetch_prompts(self.config['prompts_per_batch'])\n",
    "        \n",
    "        if self.cache:\n",
    "            self.stats['used'] += 1\n",
    "            return self.cache.pop(0)\n",
    "        return None\n",
    "    \n",
    "    def get_final_prompt(self, prompt_data):\n",
    "        \"\"\"Get the appropriate prompt based on prompt_mode.\"\"\"\n",
    "        mode = self.config['prompt_mode']\n",
    "        \n",
    "        if mode == 'dual_encoder':\n",
    "            return {\n",
    "                'type': 'dual',\n",
    "                'prompt': prompt_data.get('openclip_g') or prompt_data.get('refined') or prompt_data['full'],\n",
    "                'prompt_2': prompt_data.get('clip_l') or prompt_data.get('refined') or prompt_data['full'],\n",
    "                'negative': prompt_data['negative']\n",
    "            }\n",
    "        elif mode == 'refined' and prompt_data.get('refined'):\n",
    "            return {\n",
    "                'type': 'single',\n",
    "                'prompt': prompt_data['refined'],\n",
    "                'negative': prompt_data['negative']\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'type': 'single',\n",
    "                'prompt': prompt_data['full'],\n",
    "                'negative': prompt_data['negative']\n",
    "            }\n",
    "\n",
    "def get_resolution():\n",
    "    \"\"\"Get random resolution based on config.\"\"\"\n",
    "    if config['resolution_mode'] == 'classic':\n",
    "        return (random.choice(CLASSIC_RESOLUTIONS), random.choice(CLASSIC_RESOLUTIONS))\n",
    "    return random.choice(SDXL_RESOLUTIONS)\n",
    "\n",
    "# Initialize\n",
    "prompt_manager = PromptManager(API_BASE, config)\n",
    "print(f\"Pre-fetching {config['prompts_per_batch']} prompts...\")\n",
    "fetched = prompt_manager.fetch_prompts(config['prompts_per_batch'])\n",
    "print(f\"\\u2713 Fetched {fetched} prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Generation Loop\n",
    "metadata_list = []\n",
    "metadata_path = os.path.join(save_directory, \"metadata.json\")\n",
    "\n",
    "num_images = config['num_images']\n",
    "total_images = num_images * len(selected_schedulers)\n",
    "\n",
    "print(f\"Starting generation: {num_images} prompts x {len(selected_schedulers)} schedulers = {total_images} images\")\n",
    "print(f\"Save directory: {save_directory}\\n\")\n",
    "\n",
    "image_counter = 0\n",
    "for i in range(num_images):\n",
    "    try:\n",
    "        # Get prompt\n",
    "        prompt_data = prompt_manager.get_prompt()\n",
    "        if not prompt_data:\n",
    "            print(f\"[{i+1}/{num_images}] Failed to get prompt, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Get final prompt based on mode\n",
    "        final = prompt_manager.get_final_prompt(prompt_data)\n",
    "        \n",
    "        # Random parameters (same for all schedulers)\n",
    "        width, height = get_resolution()\n",
    "        guidance_scale = round(random.uniform(config['guidance_low'], config['guidance_high']) * 2) / 2\n",
    "        num_steps = random.randint(config['steps_low'], config['steps_high'])\n",
    "        \n",
    "        # Generate with each scheduler\n",
    "        for scheduler_name in selected_schedulers:\n",
    "            try:\n",
    "                pipe.scheduler = SAMPLERS[scheduler_name]()\n",
    "                seed = random.randint(0, 2**32 - 1)\n",
    "                generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "                \n",
    "                # Generate based on prompt type\n",
    "                if final['type'] == 'dual':\n",
    "                    # Dual-encoder mode: pass separate prompts\n",
    "                    result = pipe(\n",
    "                        prompt=final['prompt'],\n",
    "                        prompt_2=final['prompt_2'],\n",
    "                        negative_prompt=final['negative'],\n",
    "                        width=width,\n",
    "                        height=height,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        num_inference_steps=num_steps,\n",
    "                        generator=generator,\n",
    "                    ).images[0]\n",
    "                else:\n",
    "                    # Single prompt mode: use Compel for weighting\n",
    "                    conditioning = compel(final['prompt'], negative_prompt=final['negative'])\n",
    "                    result = pipe(\n",
    "                        prompt_embeds=conditioning.embeds,\n",
    "                        pooled_prompt_embeds=conditioning.pooled_embeds,\n",
    "                        negative_prompt_embeds=conditioning.negative_embeds,\n",
    "                        negative_pooled_prompt_embeds=conditioning.negative_pooled_embeds,\n",
    "                        width=width,\n",
    "                        height=height,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        num_inference_steps=num_steps,\n",
    "                        generator=generator,\n",
    "                    ).images[0]\n",
    "                \n",
    "                # Save image\n",
    "                scheduler_short = scheduler_name.replace(' ', '_').replace('+', 'p')\n",
    "                filename = f\"{datetime.now().strftime('%Y%m%d%H%M%S')}_{str(image_counter).zfill(4)}_{scheduler_short}.png\"\n",
    "                result.save(os.path.join(save_directory, filename))\n",
    "                \n",
    "                # Save metadata\n",
    "                metadata = {\n",
    "                    \"filename\": filename,\n",
    "                    \"model\": config['model_id'],\n",
    "                    \"scheduler\": scheduler_name,\n",
    "                    \"prompt_mode\": config['prompt_mode'],\n",
    "                    \"prompt\": final['prompt'],\n",
    "                    \"prompt_2\": final.get('prompt_2', ''),\n",
    "                    \"negative_prompt\": final['negative'],\n",
    "                    \"sfw\": config['sfw'],\n",
    "                    \"seed\": seed,\n",
    "                    \"width\": width,\n",
    "                    \"height\": height,\n",
    "                    \"guidance_scale\": guidance_scale,\n",
    "                    \"num_steps\": num_steps,\n",
    "                    \"prompt_index\": i\n",
    "                }\n",
    "                metadata_list.append(metadata)\n",
    "                \n",
    "                with open(metadata_path, 'w') as f:\n",
    "                    json.dump(metadata_list, f, indent=2)\n",
    "                \n",
    "                del result\n",
    "                torch.cuda.empty_cache()\n",
    "                image_counter += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with {scheduler_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Progress\n",
    "        print(f\"[{i+1}/{num_images}] {width}x{height} | {num_steps} steps | G:{guidance_scale:.1f} | {len(selected_schedulers)} schedulers\")\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error on image {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n\\u2713 Generation complete!\")\n",
    "print(f\"\\u2713 {len(metadata_list)} images saved to: {save_directory}\")\n",
    "print(f\"\\u2713 Metadata saved to: {metadata_path}\")\n",
    "print(f\"\\nPrompt stats: Fetched={prompt_manager.stats['fetched']}, Used={prompt_manager.stats['used']}, Failed={prompt_manager.stats['failed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Display Results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_columns = 3\n",
    "max_images = 12\n",
    "\n",
    "if metadata_list:\n",
    "    display_meta = metadata_list[:max_images]\n",
    "    num_rows = (len(display_meta) + num_columns - 1) // num_columns\n",
    "    \n",
    "    plt.figure(figsize=(5 * num_columns, 5 * num_rows))\n",
    "    \n",
    "    for idx, meta in enumerate(display_meta):\n",
    "        img = Image.open(os.path.join(save_directory, meta['filename']))\n",
    "        plt.subplot(num_rows, num_columns, idx + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"{meta['scheduler']}\\n{meta['width']}x{meta['height']}\", fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Displayed {len(display_meta)} of {len(metadata_list)} images\")\n",
    "else:\n",
    "    print(\"No images to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Cleanup\n",
    "del pipe, compel\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\u2713 Cleanup complete\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
