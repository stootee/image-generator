{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgS-fRFOiUNk"
      },
      "source": [
        "# Image Generator with Compel\n",
        "\n",
        "This notebook enhances the basic generator with:\n",
        "- **Compel Integration**: Handle long prompts with weighted embeddings (no 77 token limit)\n",
        "- **Batch Prompt Fetching**: Pre-fetch multiple prompts to reduce API calls\n",
        "- **Overflow Management**: Smart usage of overflow tags from prompt API\n",
        "- **Memory Optimized**: Works on Colab free tier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1jtsQq4iUNm",
        "outputId": "533e2d5c-1035-4460-a252-f0c84580d76b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import drive\n",
        "\n",
        "mount_path = '/content/drive'\n",
        "\n",
        "if not os.path.exists(mount_path):\n",
        "    print(\"Drive not mounted. Mounting now...\")\n",
        "    drive.mount(mount_path)\n",
        "else:\n",
        "    print(\"Drive already mounted.\")\n",
        "\n",
        "# Define the path to your file\n",
        "file_path = '/content/drive/MyDrive/AI/hf_token.env'\n",
        "\n",
        "# Load the environment variables\n",
        "load_dotenv(file_path)\n",
        "\n",
        "huggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='Flax classes are deprecated')\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QPxql7TziUNn"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q diffusers transformers accelerate safetensors omegaconf invisible-watermark compel bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK2c7ZnviUNn",
        "outputId": "5d101b86-f192-4525-fda0-ecf22322a355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Initial GPU Memory: 7.61GB free\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "import gc\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import random\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from huggingface_hub import snapshot_download, login\n",
        "from diffusers import (\n",
        "    StableDiffusionXLPipeline,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    KDPM2AncestralDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    UniPCMultistepScheduler\n",
        ")\n",
        "from compel import CompelForSDXL\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Initial GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYIlygeriUNn"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXONf1tJiUNo",
        "outputId": "c31d306c-d83a-4108-9253-e35e79347c2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Configuration loaded\n",
            "  Model: John6666/easy-sfw-nsfw-pony-cartooneasyv1-sdxl\n",
            "  Selected schedulers (3): Euler, KDPM2, DDIM\n",
            "  Total images to generate: 50 × 3 = 150\n",
            "  Guidance: 5-15\n",
            "  Steps: 25-40\n",
            "  Resolution: sdxl\n",
            "  Uniform beta schedule: False\n",
            "  Save to: /content/drive/MyDrive/AI/images/20260101231503/\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### Paths and Authentication\n",
        "base_path = \"/content/drive/MyDrive/AI/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Model Configuration\n",
        "model_id = \"John6666/easy-sfw-nsfw-pony-cartooneasyv1-sdxl\" #@param [\"stablediffusionapi/mklan-xxx-nsfw-pony\",\"stablediffusionapi/duchaiten-real3d-nsfw-xl\", \"John6666/cyberrealistic-pony-v7-sdxl\", \"John6666/uber-realistic-porn-merge-xl-urpmxl-v6final-sdxl\",\"John6666/fucktastic-real-checkpoint-pony-pdxl-porn-realistic-nsfw-sfw-21-sdxl\",\"Vvilams/pony-realism-v21main-sdxl\",\"John6666/sexoholic-real-pony-nsfw-v2-sdxl\",\"John6666/wai-ani-nsfw-ponyxl-v11-sdxl\", \"John6666/duchaiten-pony-real-v20-sdxl\", \"stable-diffusion-v1-5/stable-diffusion-v1-5\", \"stabilityai/stable-diffusion-xl-base-1.0\", \"John6666/pornworks-real-porn-v03-sdxl\", \"UnfilteredAI/NSFW-GEN-ANIME\", \"UnfilteredAI/NSFW-gen-v2\", \"John6666/easy-sfw-nsfw-pony-cartooneasyv1-sdxl\", \"John6666/wai-nsfw-illustrious-v80-sdxl\"]\n",
        "download_model = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Prompt Configuration\n",
        "base_prompt = \"\" #@param {type:\"string\"}\n",
        "sfw = False #@param {type:\"boolean\"}\n",
        "selected_categories = [] #@param {type:\"raw\"}\n",
        "prompts_per_batch = 2 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Generation Settings\n",
        "num_of_images = 50 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown #### Schedulers (select one or more)\n",
        "use_model_default = False #@param {type:\"boolean\"}\n",
        "use_dpm_2m = False #@param {type:\"boolean\"}\n",
        "use_dpm_2m_karras = False #@param {type:\"boolean\"}\n",
        "use_dpm_sde = False #@param {type:\"boolean\"}\n",
        "use_dpm_sde_karras = False #@param {type:\"boolean\"}\n",
        "use_euler = True #@param {type:\"boolean\"}\n",
        "use_euler_a = False #@param {type:\"boolean\"}\n",
        "use_heun = False #@param {type:\"boolean\"}\n",
        "use_kdpm2 = True #@param {type:\"boolean\"}\n",
        "use_kdpm2_a = False #@param {type:\"boolean\"}\n",
        "use_lms = False #@param {type:\"boolean\"}\n",
        "use_ddim = True #@param {type:\"boolean\"}\n",
        "use_pndm = False #@param {type:\"boolean\"}\n",
        "use_unipc = False #@param {type:\"boolean\"}\n",
        "\n",
        "guidance_low = 5 #@param {type:\"integer\"}\n",
        "guidance_high = 15 #@param {type:\"integer\"}\n",
        "steps_low = 25 #@param {type:\"integer\"}\n",
        "steps_high = 40 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Resolution Settings\n",
        "resolution_mode = \"sdxl\" #@param [\"classic\", \"sdxl\"]\n",
        "\n",
        "# Resolution pools\n",
        "CLASSIC_RESOLUTIONS = [720, 768, 800, 1024]\n",
        "SDXL_RESOLUTIONS = [\n",
        "    (1024, 1024), (1152, 896), (896, 1152), (1216, 832),\n",
        "    (832, 1216), (1344, 768), (768, 1344)\n",
        "]\n",
        "\n",
        "# Build selected schedulers list\n",
        "selected_schedulers = []\n",
        "if use_model_default: selected_schedulers.append(\"Model Default\")\n",
        "if use_dpm_2m: selected_schedulers.append(\"DPM++ 2M\")\n",
        "if use_dpm_2m_karras: selected_schedulers.append(\"DPM++ 2M Karras\")\n",
        "if use_dpm_sde: selected_schedulers.append(\"DPM++ SDE\")\n",
        "if use_dpm_sde_karras: selected_schedulers.append(\"DPM++ SDE Karras\")\n",
        "if use_euler: selected_schedulers.append(\"Euler\")\n",
        "if use_euler_a: selected_schedulers.append(\"Euler a\")\n",
        "if use_heun: selected_schedulers.append(\"Heun\")\n",
        "if use_kdpm2: selected_schedulers.append(\"KDPM2\")\n",
        "if use_kdpm2_a: selected_schedulers.append(\"KDPM2 a\")\n",
        "if use_lms: selected_schedulers.append(\"LMS\")\n",
        "if use_ddim: selected_schedulers.append(\"DDIM\")\n",
        "if use_pndm: selected_schedulers.append(\"PNDM\")\n",
        "if use_unipc: selected_schedulers.append(\"UniPC\")\n",
        "\n",
        "# Validate at least one scheduler selected\n",
        "if not selected_schedulers:\n",
        "    raise ValueError(\"ERROR: At least one scheduler must be selected!\")\n",
        "\n",
        "# Derived paths\n",
        "model_path = base_path + \"models/\" + model_id\n",
        "save_directory = f\"{base_path}images/{datetime.now().strftime('%Y%m%d%H%M%S')}/\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# Models requiring uniform beta_schedule instead of scaled_linear\n",
        "uniform_models = [\n",
        "    \"John6666/uber-realistic-porn-merge-xl-urpmxl-v6final-sdxl\",\n",
        "    \"John6666/sexoholic-real-pony-nsfw-v2-sdxl\",\n",
        "    \"John6666/duchaiten-pony-real-v20-sdxl\",\n",
        "    \"stabilityai/sdxl-turbo\",\n",
        "    \"John6666/pornworks-real-porn-v03-sdxl\"\n",
        "]\n",
        "use_uniform = model_id in uniform_models\n",
        "\n",
        "# Fallback negative prompt (API now provides negative_prompt per image)\n",
        "NEGATIVE_PROMPT = \"text, writing, bad teeth, deformed face and eyes, child, childish, young, deformed, uneven eyes, too many fingers\"\n",
        "\n",
        "print(f\"✓ Configuration loaded\")\n",
        "print(f\"  Model: {model_id}\")\n",
        "print(f\"  Selected schedulers ({len(selected_schedulers)}): {', '.join(selected_schedulers)}\")\n",
        "print(f\"  Total images to generate: {num_of_images} × {len(selected_schedulers)} = {num_of_images * len(selected_schedulers)}\")\n",
        "print(f\"  Guidance: {guidance_low}-{guidance_high}\")\n",
        "print(f\"  Steps: {steps_low}-{steps_high}\")\n",
        "print(f\"  Resolution: {resolution_mode}\")\n",
        "print(f\"  Uniform beta schedule: {use_uniform}\")\n",
        "print(f\"  Save to: {save_directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN5WT1lciUNo"
      },
      "source": [
        "## Prompt Management System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVu--V7KiUNp",
        "outputId": "b51e3a64-5dbe-4166-91db-431ce66a39c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-fetching 2 prompts...\n",
            "Fetched 2 prompts successfully\n"
          ]
        }
      ],
      "source": [
        "class PromptManager:\n",
        "    \"\"\"Manages prompt fetching, caching, and variation generation.\"\"\"\n",
        "\n",
        "    def __init__(self, api_url_template, cache_size=20):\n",
        "        self.api_url_template = api_url_template\n",
        "        self.cache = []\n",
        "        self.cache_size = cache_size\n",
        "        self.stats = defaultdict(int)\n",
        "\n",
        "    def fetch_prompts(self, count=5):\n",
        "        \"\"\"Fetch multiple prompts at once and cache them.\"\"\"\n",
        "        prompts = []\n",
        "        for _ in range(count):\n",
        "            try:\n",
        "                response = requests.get(self.api_url_template, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    # Parse overflow - API returns comma-separated string, not array\n",
        "                    overflow_raw = data.get('overflow', '')\n",
        "                    if isinstance(overflow_raw, str):\n",
        "                        # Split by comma and strip whitespace\n",
        "                        overflow = [item.strip() for item in overflow_raw.split(',') if item.strip()]\n",
        "                    elif isinstance(overflow_raw, list):\n",
        "                        # Already a list (in case API changes)\n",
        "                        overflow = overflow_raw\n",
        "                    else:\n",
        "                        overflow = []\n",
        "\n",
        "                    prompts.append({\n",
        "                        'prompt': data['prompt'],\n",
        "                        'overflow': overflow,\n",
        "                        'refined': data.get('refined', ''),\n",
        "                        'negative_prompt': data.get('negative_prompt', NEGATIVE_PROMPT)  # Use API's negative or fallback\n",
        "                    })\n",
        "                    self.stats['fetched'] += 1\n",
        "                else:\n",
        "                    print(f\"Failed to fetch prompt: {response.status_code}\")\n",
        "                    self.stats['failed'] += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching prompt: {e}\")\n",
        "                self.stats['failed'] += 1\n",
        "\n",
        "        self.cache.extend(prompts)\n",
        "        # Keep cache size manageable\n",
        "        if len(self.cache) > self.cache_size:\n",
        "            self.cache = self.cache[-self.cache_size:]\n",
        "\n",
        "        return len(prompts)\n",
        "\n",
        "    def get_prompt(self):\n",
        "        \"\"\"Get a prompt from cache or fetch new ones if cache is low.\"\"\"\n",
        "        if len(self.cache) < 3:\n",
        "            print(f\"Cache low ({len(self.cache)} prompts), fetching more...\")\n",
        "            self.fetch_prompts(prompts_per_batch)\n",
        "\n",
        "        if not self.cache:\n",
        "            # Emergency fetch\n",
        "            self.fetch_prompts(1)\n",
        "\n",
        "        if self.cache:\n",
        "            self.stats['used'] += 1\n",
        "            return self.cache.pop(0)\n",
        "        return None\n",
        "\n",
        "    def build_weighted_prompt(self, prompt_data):\n",
        "        \"\"\"\n",
        "        Build weighted prompt using all overflow tags.\n",
        "\n",
        "        Args:\n",
        "            prompt_data: Dict with 'prompt', 'overflow', 'refined', 'negative_prompt'\n",
        "\n",
        "        Returns:\n",
        "            tuple: (final_prompt, overflow_count)\n",
        "        \"\"\"\n",
        "        prompt = prompt_data['prompt']\n",
        "        overflow = prompt_data.get('overflow', [])\n",
        "\n",
        "        if not overflow:\n",
        "            return prompt, 0\n",
        "\n",
        "        # Use ALL overflow tags (API provides them for a reason)\n",
        "        overflow_text = \", \".join(overflow)\n",
        "        final_prompt = f\"{prompt}, {overflow_text}\"\n",
        "\n",
        "        return final_prompt, len(overflow)\n",
        "\n",
        "    def print_stats(self):\n",
        "        print(f\"\\nPrompt Manager Stats:\")\n",
        "        print(f\"  Fetched: {self.stats['fetched']}\")\n",
        "        print(f\"  Used: {self.stats['used']}\")\n",
        "        print(f\"  Failed: {self.stats['failed']}\")\n",
        "        print(f\"  Cached: {len(self.cache)}\")\n",
        "\n",
        "# Initialize prompt manager\n",
        "prompt_url = f\"https://prompt-gen.squigglypickle.co.uk/generate-prompt?sfw={sfw}&base_prompt={base_prompt}&selected_categories={' '.join(selected_categories)}\"\n",
        "prompt_manager = PromptManager(prompt_url)\n",
        "\n",
        "# Pre-fetch prompts\n",
        "print(f\"Pre-fetching {prompts_per_batch} prompts...\")\n",
        "fetched = prompt_manager.fetch_prompts(prompts_per_batch)\n",
        "print(f\"Fetched {fetched} prompts successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "8d12cd29326048908d81f9d7452d0af7",
            "24de07a45f194cb29b122671009257e8",
            "202d806c538f4079990135bc055f49ea",
            "672401dfae8d4e74aff38bae7a3dc0dd",
            "60d551f6a246425bba20472a3b67609a",
            "d1e96e987706405282d9458e4142298c",
            "de7ebfd4c5fe499f83d123eff55aa20a",
            "99d7ee57082a4dae95349ead6f3c06ca",
            "80ec41aa290e497397cf4bc51d3e15c8",
            "6b526570ff724dc1a88dd2c515f75c4b",
            "1c3746593bdd4c8b9de9353416c8646e"
          ]
        },
        "id": "8eHhc0oLiUNp",
        "outputId": "0fdd693e-e6bf-4b83-c812-cbb819d85f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model already exists at /content/drive/MyDrive/AI/models/John6666/easy-sfw-nsfw-pony-cartooneasyv1-sdxl, skipping download\n",
            "Loading model John6666/easy-sfw-nsfw-pony-cartooneasyv1-sdxl...\n",
            "Attempting to load with fp16 variant...\n",
            "Retrying without variant (using full precision weights)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d12cd29326048908d81f9d7452d0af7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded from local path: /content/drive/MyDrive/AI/models/John6666/easy-sfw-nsfw-pony-cartooneasyv1-sdxl\n",
            "✓ Model loaded successfully (no_variant)\n",
            "\n",
            "Loading enhanced VAE for better color accuracy...\n",
            "✓ Enhanced VAE loaded from HuggingFace\n",
            "✓ Enhanced VAE applied to pipeline\n"
          ]
        }
      ],
      "source": [
        "# Check if model needs to be downloaded\n",
        "model_exists = os.path.exists(model_path) and os.path.isdir(model_path) and len(os.listdir(model_path)) > 0\n",
        "should_download = download_model or not model_exists\n",
        "\n",
        "if should_download:\n",
        "    if not model_exists:\n",
        "        print(f\"Model not found at {model_path}, downloading from HuggingFace...\")\n",
        "    elif download_model:\n",
        "        print(f\"Re-downloading model {model_id} (download_model checkbox enabled)...\")\n",
        "\n",
        "    if not huggingface_token:\n",
        "        print(\"Warning: No HuggingFace token provided. Download may fail for gated models.\")\n",
        "\n",
        "    snapshot_download(\n",
        "        repo_id=model_id,\n",
        "        local_dir=model_path,\n",
        "        token=huggingface_token if huggingface_token else None,\n",
        "        ignore_patterns=[\"*.safetensors.lock\"]\n",
        "    )\n",
        "    print(f\"✓ Model downloaded to: {model_path}\")\n",
        "else:\n",
        "    print(f\"✓ Model already exists at {model_path}, skipping download\")\n",
        "\n",
        "# Load pipeline with memory optimizations\n",
        "print(f\"Loading model {model_id}...\")\n",
        "\n",
        "# Base load kwargs\n",
        "load_kwargs = {\n",
        "    \"torch_dtype\": torch.float16,\n",
        "    \"use_safetensors\": True,\n",
        "}\n",
        "\n",
        "# Try loading with fp16 variant first, fallback to no variant if unavailable\n",
        "pipe = None\n",
        "for attempt in [\"fp16\", \"no_variant\"]:\n",
        "    try:\n",
        "        if attempt == \"fp16\":\n",
        "            load_kwargs[\"variant\"] = \"fp16\"\n",
        "            print(\"Attempting to load with fp16 variant...\")\n",
        "        else:\n",
        "            load_kwargs.pop(\"variant\", None)\n",
        "            print(\"Retrying without variant (using full precision weights)...\")\n",
        "\n",
        "        # Try loading from local path first, fall back to HuggingFace if corrupted\n",
        "        try:\n",
        "            if os.path.exists(model_path):\n",
        "                pipe = StableDiffusionXLPipeline.from_pretrained(model_path, **load_kwargs)\n",
        "                print(f\"✓ Loaded from local path: {model_path}\")\n",
        "            else:\n",
        "                raise FileNotFoundError(\"Local model path does not exist\")\n",
        "        except (OSError, FileNotFoundError) as local_error:\n",
        "            # Local model is corrupted or missing, load from HuggingFace\n",
        "            if os.path.exists(model_path):\n",
        "                print(f\"⚠ Local model corrupted: {local_error}\")\n",
        "                print(f\"Loading from HuggingFace instead...\")\n",
        "            load_kwargs[\"token\"] = huggingface_token if huggingface_token else None\n",
        "            pipe = StableDiffusionXLPipeline.from_pretrained(model_id, **load_kwargs)\n",
        "            print(f\"✓ Loaded from HuggingFace: {model_id}\")\n",
        "\n",
        "        print(f\"✓ Model loaded successfully ({attempt})\")\n",
        "        break\n",
        "\n",
        "    except ValueError as e:\n",
        "        if \"variant\" in str(e) and attempt == \"fp16\":\n",
        "            # fp16 variant not available, will retry without variant\n",
        "            continue\n",
        "        else:\n",
        "            # Some other ValueError, re-raise it\n",
        "            raise\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model with {attempt}: {e}\")\n",
        "        if attempt == \"no_variant\":\n",
        "            # Last attempt failed, re-raise\n",
        "            raise\n",
        "\n",
        "if pipe is None:\n",
        "    raise RuntimeError(\"Failed to load model after all attempts\")\n",
        "\n",
        "# NOW load the enhanced VAE (after pipeline is loaded)\n",
        "# This ensures we can properly replace the model's default VAE\n",
        "print(\"\\nLoading enhanced VAE for better color accuracy...\")\n",
        "from diffusers import AutoencoderKL\n",
        "\n",
        "try:\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        \"madebyollin/sdxl-vae-fp16-fix\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    print(\"✓ Enhanced VAE loaded from HuggingFace\")\n",
        "\n",
        "    # Replace the pipeline's VAE with the enhanced one\n",
        "    pipe.vae = vae\n",
        "    print(\"✓ Enhanced VAE applied to pipeline\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Could not load enhanced VAE, using model's default: {e}\")\n",
        "\n",
        "# Move pipeline to GPU\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# Memory optimizations\n",
        "print(\"\\nApplying memory optimizations...\")\n",
        "pipe.enable_attention_slicing()\n",
        "pipe.enable_vae_slicing()\n",
        "pipe.enable_vae_tiling()\n",
        "\n",
        "# Try to enable xformers for better memory efficiency\n",
        "try:\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "    print(\"✓ xformers enabled\")\n",
        "except:\n",
        "    print(\"⚠ xformers not available\")\n",
        "\n",
        "# NOTE: enable_model_cpu_offload() is DISABLED because it conflicts with Compel\n",
        "# Compel needs text encoders to stay on GPU for prompt weighting\n",
        "print(\"⚠ CPU offload disabled (incompatible with Compel)\")\n",
        "\n",
        "# Initialize CompelForSDXL for advanced prompt weighting\n",
        "# This wrapper automatically handles SDXL's dual text encoders and pooled embeddings\n",
        "compel = CompelForSDXL(pipe)\n",
        "print(\"✓ CompelForSDXL initialized for advanced prompt weighting\")\n",
        "\n",
        "print(f\"\\n✓ Pipeline ready!\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGXveRB4iUNp"
      },
      "source": [
        "# Sampler Configuration (simplified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_7mbhnjiUNp"
      },
      "outputs": [],
      "source": [
        "# Sampler classes\n",
        "from diffusers import (\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DPMSolverSinglestepScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    KDPM2DiscreteScheduler,\n",
        "    KDPM2AncestralDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    DDIMScheduler,\n",
        "    PNDMScheduler,\n",
        "    UniPCMultistepScheduler\n",
        ")\n",
        "\n",
        "# Apply uniform beta_schedule for models that require it\n",
        "if use_uniform:\n",
        "    print(f\"Applying uniform beta_schedule for {model_id}...\")\n",
        "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "    pipe.scheduler.config.beta_schedule = \"uniform\"\n",
        "    print(f\"✓ Scheduler set to DPMSolverMultistepScheduler with uniform beta_schedule\")\n",
        "\n",
        "# IMPORTANT: Initialize samplers AFTER pipeline is loaded so we can use the model's scheduler config\n",
        "# This preserves the model's trained scheduler settings (beta schedules, timestep spacing, etc.)\n",
        "\n",
        "def initialize_samplers(base_scheduler_config):\n",
        "    \"\"\"\n",
        "    Create scheduler instances using the model's trained config.\n",
        "    This ensures we inherit important settings like beta_schedule, timestep_spacing, etc.\n",
        "\n",
        "    IMPORTANT: For Karras variants, we use a clean config to avoid conflicts between\n",
        "    beta_schedule settings and Karras sigmas (which override beta schedules anyway).\n",
        "    This prevents duplicate timestep bugs that cause index out of bounds errors.\n",
        "    \"\"\"\n",
        "    # Create a clean config for Karras variants (without beta_schedule that might conflict)\n",
        "    clean_config = {k: v for k, v in base_scheduler_config.items() if k not in ['beta_schedule']}\n",
        "\n",
        "    return {\n",
        "        # DPM++ variants (multistep)\n",
        "        \"DPM++ 2M\": lambda: DPMSolverMultistepScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"DPM++ 2M Karras\": lambda: DPMSolverMultistepScheduler.from_config(\n",
        "            clean_config,\n",
        "            use_karras_sigmas=True\n",
        "        ),\n",
        "\n",
        "        # DPM++ SDE variants (singlestep - stochastic)\n",
        "        \"DPM++ SDE\": lambda: DPMSolverSinglestepScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"DPM++ SDE Karras\": lambda: DPMSolverSinglestepScheduler.from_config(\n",
        "            clean_config,\n",
        "            use_karras_sigmas=True\n",
        "        ),\n",
        "\n",
        "        # Euler variants\n",
        "        \"Euler\": lambda: EulerDiscreteScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"Euler a\": lambda: EulerAncestralDiscreteScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "\n",
        "        # KDPM2 variants\n",
        "        \"KDPM2\": lambda: KDPM2DiscreteScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"KDPM2 a\": lambda: KDPM2AncestralDiscreteScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "\n",
        "        # Other popular schedulers\n",
        "        \"Heun\": lambda: HeunDiscreteScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"LMS\": lambda: LMSDiscreteScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"DDIM\": lambda: DDIMScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"PNDM\": lambda: PNDMScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"UniPC\": lambda: UniPCMultistepScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "\n",
        "        # Model's original scheduler\n",
        "        \"Model Default\": lambda: pipe.scheduler.__class__.from_config(\n",
        "            base_scheduler_config\n",
        "        )\n",
        "    }\n",
        "\n",
        "# Initialize samplers with the model's scheduler config\n",
        "SAMPLERS = initialize_samplers(pipe.scheduler.config)\n",
        "\n",
        "def get_resolution():\n",
        "    \"\"\"Get random resolution based on mode.\"\"\"\n",
        "    if resolution_mode == \"classic\":\n",
        "        width = random.choice(CLASSIC_RESOLUTIONS)\n",
        "        height = random.choice(CLASSIC_RESOLUTIONS)\n",
        "        return (width, height)\n",
        "    else:  # sdxl\n",
        "        return random.choice(SDXL_RESOLUTIONS)\n",
        "\n",
        "print(f\"✓ Samplers configured using model's scheduler config\")\n",
        "print(f\"  Base scheduler: {pipe.scheduler.__class__.__name__}\")\n",
        "print(f\"  Available samplers: {len(SAMPLERS)}\")\n",
        "print(f\"  Config: beta_schedule={pipe.scheduler.config.get('beta_schedule', 'N/A')}, \"\n",
        "      f\"timestep_spacing={pipe.scheduler.config.get('timestep_spacing', 'N/A')}\")\n",
        "print(f\"  Karras variants use clean config (no beta_schedule) to prevent timestep conflicts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87sDLdiXRUHl"
      },
      "source": [
        "## Scheduler Comparison Test (Optional)\n",
        "\n",
        "Test all schedulers with identical parameters to compare quality and characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK2UB48ERUHl"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Scheduler Comparison Settings\n",
        "run_sampler_test = False #@param {type:\"boolean\"}\n",
        "test_prompt = \"a beautiful woman with long flowing hair, photorealistic, high quality, detailed\" #@param {type:\"string\"}\n",
        "test_seed = 42 #@param {type:\"integer\"}\n",
        "test_width = 1024 #@param {type:\"integer\"}\n",
        "test_height = 1024 #@param {type:\"integer\"}\n",
        "test_steps = 30 #@param {type:\"integer\"}\n",
        "test_guidance = 7.5 #@param {type:\"number\"}\n",
        "test_columns = 3 #@param {type:\"integer\"}\n",
        "\n",
        "if run_sampler_test:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"SCHEDULER COMPARISON TEST\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nTest Parameters:\")\n",
        "    print(f\"  Prompt: {test_prompt[:60]}...\")\n",
        "    print(f\"  Seed: {test_seed}\")\n",
        "    print(f\"  Resolution: {test_width}x{test_height}\")\n",
        "    print(f\"  Steps: {test_steps}\")\n",
        "    print(f\"  Guidance: {test_guidance}\")\n",
        "    print(f\"\\nGenerating with {len(SAMPLERS)} schedulers...\")\n",
        "\n",
        "    # Create test directory\n",
        "    test_directory = f\"{base_path}scheduler_tests/{datetime.now().strftime('%Y%m%d%H%M%S')}/\"\n",
        "    os.makedirs(test_directory, exist_ok=True)\n",
        "\n",
        "    # Encode prompt once (use for all schedulers)\n",
        "    # IMPORTANT: Pass BOTH positive and negative prompts to compel() together\n",
        "    print(\"\\nEncoding prompt...\")\n",
        "    conditioning = compel(test_prompt, negative_prompt=NEGATIVE_PROMPT)\n",
        "\n",
        "    # Generate with each scheduler\n",
        "    test_results = []\n",
        "    for scheduler_name in sorted(SAMPLERS.keys()):\n",
        "        try:\n",
        "            print(f\"\\n[{len(test_results)+1}/{len(SAMPLERS)}] Testing: {scheduler_name}\")\n",
        "\n",
        "            # Set scheduler\n",
        "            pipe.scheduler = SAMPLERS[scheduler_name]()\n",
        "\n",
        "            # Generate image\n",
        "            # IMPORTANT: Must pass all 4 embedding parameters when using compel\n",
        "            result = pipe(\n",
        "                prompt_embeds=conditioning.embeds,\n",
        "                pooled_prompt_embeds=conditioning.pooled_embeds,\n",
        "                negative_prompt_embeds=conditioning.negative_embeds,\n",
        "                negative_pooled_prompt_embeds=conditioning.negative_pooled_embeds,\n",
        "                num_images_per_prompt=1,\n",
        "                width=test_width,\n",
        "                height=test_height,\n",
        "                guidance_scale=test_guidance,\n",
        "                num_inference_steps=test_steps,\n",
        "                generator=torch.Generator(device=\"cuda\").manual_seed(test_seed),\n",
        "            ).images[0]\n",
        "\n",
        "            # Save image\n",
        "            filename = f\"{scheduler_name.replace(' ', '_').replace('+', 'p')}.png\"\n",
        "            filepath = os.path.join(test_directory, filename)\n",
        "            result.save(filepath)\n",
        "\n",
        "            test_results.append({\n",
        "                'scheduler': scheduler_name,\n",
        "                'image': result,\n",
        "                'filename': filename\n",
        "            })\n",
        "\n",
        "            print(f\"  ✓ Saved: {filename}\")\n",
        "\n",
        "            # Cleanup\n",
        "            del result\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Cleanup encoded prompt\n",
        "    del conditioning\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Display results in grid\n",
        "    if test_results:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"RESULTS: {len(test_results)} schedulers tested\")\n",
        "        print(f\"Saved to: {test_directory}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        # Create comparison grid\n",
        "        num_results = len(test_results)\n",
        "        num_rows = (num_results + test_columns - 1) // test_columns\n",
        "\n",
        "        fig = plt.figure(figsize=(6 * test_columns, 6 * num_rows))\n",
        "\n",
        "        for idx, result in enumerate(test_results):\n",
        "            ax = plt.subplot(num_rows, test_columns, idx + 1)\n",
        "            ax.imshow(result['image'])\n",
        "            ax.axis('off')\n",
        "            ax.set_title(result['scheduler'], fontsize=12, weight='bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            'prompt': test_prompt,\n",
        "            'negative_prompt': NEGATIVE_PROMPT,\n",
        "            'seed': test_seed,\n",
        "            'width': test_width,\n",
        "            'height': test_height,\n",
        "            'steps': test_steps,\n",
        "            'guidance_scale': test_guidance,\n",
        "            'model': model_id,\n",
        "            'schedulers_tested': [r['scheduler'] for r in test_results],\n",
        "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(test_directory, 'test_metadata.json'), 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "\n",
        "        print(f\"\\nℹ️  To compare specific schedulers, look at the images in:\")\n",
        "        print(f\"   {test_directory}\")\n",
        "    else:\n",
        "        print(\"\\n✗ No results generated\")\n",
        "else:\n",
        "    print(\"ℹ️  Scheduler comparison test disabled\")\n",
        "    print(\"   Set run_sampler_test = True to compare all schedulers with identical parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y162nF67iUNq"
      },
      "source": [
        "# Image Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqu6Jz-7iUNq"
      },
      "outputs": [],
      "source": [
        "metadata_list = []\n",
        "metadata_path = os.path.join(save_directory, \"metadata.json\")\n",
        "\n",
        "print(f\"Starting generation of {num_of_images} images with {len(selected_schedulers)} scheduler(s)...\")\n",
        "print(f\"Total images: {num_of_images * len(selected_schedulers)}\")\n",
        "print(f\"Save directory: {save_directory}\\n\")\n",
        "\n",
        "image_counter = 0\n",
        "for i in range(num_of_images):\n",
        "    try:\n",
        "        # Get prompt from manager\n",
        "        prompt_data = prompt_manager.get_prompt()\n",
        "        if not prompt_data:\n",
        "            print(f\"Failed to get prompt for image {i}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Build final prompt with ALL overflow tags\n",
        "        final_prompt, overflow_count = prompt_manager.build_weighted_prompt(prompt_data)\n",
        "\n",
        "        # Randomize parameters (same for all schedulers)\n",
        "        width, height = get_resolution()\n",
        "        guidance_scale = round(random.uniform(guidance_low, guidance_high) * 2) / 2\n",
        "        num_steps = random.randint(steps_low, steps_high)\n",
        "\n",
        "        # Use CompelForSDXL for prompt encoding (handles long prompts and SDXL dual encoders)\n",
        "        # IMPORTANT: Pass BOTH positive and negative prompts to compel() together\n",
        "        negative_prompt_text = prompt_data.get('negative_prompt', NEGATIVE_PROMPT)\n",
        "        conditioning = compel(final_prompt, negative_prompt=negative_prompt_text)\n",
        "\n",
        "        # Generate with each selected scheduler\n",
        "        for scheduler_name in selected_schedulers:\n",
        "            try:\n",
        "                # Set scheduler\n",
        "                pipe.scheduler = SAMPLERS[scheduler_name]()\n",
        "\n",
        "                seed = random.randint(0, 2**32 - 1)\n",
        "\n",
        "                # Generate image\n",
        "                # IMPORTANT: Must pass all 4 embedding parameters when using compel\n",
        "                result = pipe(\n",
        "                    prompt_embeds=conditioning.embeds,\n",
        "                    pooled_prompt_embeds=conditioning.pooled_embeds,\n",
        "                    negative_prompt_embeds=conditioning.negative_embeds,\n",
        "                    negative_pooled_prompt_embeds=conditioning.negative_pooled_embeds,\n",
        "                    num_images_per_prompt=1,\n",
        "                    width=width,\n",
        "                    height=height,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    num_inference_steps=num_steps,\n",
        "                    generator=torch.Generator(device=\"cuda\").manual_seed(seed),\n",
        "                ).images[0]\n",
        "\n",
        "                # Save image with scheduler name in filename\n",
        "                scheduler_short = scheduler_name.replace(' ', '_').replace('+', 'p')\n",
        "                filename = f\"{datetime.now().strftime('%Y%m%d%H%M%S')}_{str(image_counter).zfill(4)}_{scheduler_short}.png\"\n",
        "                filepath = os.path.join(save_directory, filename)\n",
        "                result.save(filepath)\n",
        "\n",
        "                # Store metadata\n",
        "                metadata = {\n",
        "                    \"filename\": filename,\n",
        "                    \"model\": model_id,\n",
        "                    \"scheduler\": scheduler_name,\n",
        "                    \"base_prompt\": base_prompt,\n",
        "                    \"prompt\": prompt_data['prompt'],\n",
        "                    \"final_prompt\": final_prompt,\n",
        "                    \"overflow_total\": len(prompt_data.get('overflow', [])),\n",
        "                    \"overflow_used\": overflow_count,\n",
        "                    \"negative_prompt\": negative_prompt_text,\n",
        "                    \"sfw\": sfw,\n",
        "                    \"seed\": seed,\n",
        "                    \"width\": width,\n",
        "                    \"height\": height,\n",
        "                    \"guidance_scale\": guidance_scale,\n",
        "                    \"num_steps\": num_steps,\n",
        "                    \"prompt_index\": i\n",
        "                }\n",
        "                metadata_list.append(metadata)\n",
        "\n",
        "                # Save metadata after each image (in case of crash/failure)\n",
        "                with open(metadata_path, 'w') as f:\n",
        "                    json.dump(metadata_list, f, indent=2)\n",
        "\n",
        "                # Memory cleanup\n",
        "                del result\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                image_counter += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error with scheduler {scheduler_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Progress update\n",
        "        overflow_info = f\" +{overflow_count} overflow\" if overflow_count > 0 else \"\"\n",
        "        schedulers_info = f\"{len(selected_schedulers)} schedulers\" if len(selected_schedulers) > 1 else selected_schedulers[0]\n",
        "        print(f\"[{i+1}/{num_of_images}] {width}x{height} | {num_steps} steps | G:{guidance_scale:.1f} | {schedulers_info}{overflow_info}\")\n",
        "\n",
        "        # Cleanup prompt embeddings\n",
        "        del conditioning\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image {i}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        continue\n",
        "\n",
        "print(f\"\\n✓ Generation complete!\")\n",
        "print(f\"✓ {len(metadata_list)} images saved to: {save_directory}\")\n",
        "print(f\"✓ Metadata saved to: {metadata_path}\")\n",
        "\n",
        "# Print stats\n",
        "prompt_manager.print_stats()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nFinal GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0fTFpZxiUNq"
      },
      "source": [
        "## Statistics and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAd9ct_iiUNq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "if not run_sampler_test and 'metadata_list' in locals() and metadata_list:\n",
        "    # Sampler distribution\n",
        "    samplers = [m['sampler'] for m in metadata_list]\n",
        "    sampler_counts = Counter(samplers)\n",
        "\n",
        "    # Resolution distribution\n",
        "    resolutions = [f\"{m['width']}x{m['height']}\" for m in metadata_list]\n",
        "    resolution_counts = Counter(resolutions)\n",
        "\n",
        "    # Overflow usage\n",
        "    avg_overflow_used = sum(m['overflow_used'] for m in metadata_list) / len(metadata_list)\n",
        "    avg_overflow_total = sum(m['overflow_total'] for m in metadata_list) / len(metadata_list)\n",
        "\n",
        "    print(\"\\n=== Generation Statistics ===\")\n",
        "    print(f\"\\nSampler Usage:\")\n",
        "    for sampler, count in sampler_counts.most_common():\n",
        "        print(f\"  {sampler}: {count} ({count/len(metadata_list)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nTop 5 Resolutions:\")\n",
        "    for res, count in resolution_counts.most_common(5):\n",
        "        print(f\"  {res}: {count} ({count/len(metadata_list)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nOverflow Usage:\")\n",
        "    print(f\"  Average used: {avg_overflow_used:.1f}\")\n",
        "    print(f\"  Average total: {avg_overflow_total:.1f}\")\n",
        "    print(f\"  Usage rate: {avg_overflow_used/avg_overflow_total*100:.1f}%\" if avg_overflow_total > 0 else \"  N/A\")\n",
        "elif run_sampler_test:\n",
        "    print(\"ℹ️  Statistics skipped (sampler test mode - see test results above)\")\n",
        "else:\n",
        "    print(\"ℹ️  No metadata available for statistics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V9JGiQwiUNq"
      },
      "source": [
        "## Display Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vCC94IoiUNq"
      },
      "outputs": [],
      "source": [
        "#@markdown Display generated images in a grid\n",
        "show_results = True #@param {type:\"boolean\"}\n",
        "num_columns = 3 #@param {type:\"integer\"}\n",
        "max_images_to_show = 12 #@param {type:\"integer\"}\n",
        "\n",
        "if not run_sampler_test and show_results and 'metadata_list' in locals() and metadata_list:\n",
        "    display_metadata = metadata_list[:max_images_to_show]\n",
        "    num_images = len(display_metadata)\n",
        "    num_rows = (num_images + num_columns - 1) // num_columns\n",
        "\n",
        "    plt.figure(figsize=(5 * num_columns, 5 * num_rows))\n",
        "\n",
        "    for idx, metadata in enumerate(display_metadata):\n",
        "        filepath = os.path.join(save_directory, metadata['filename'])\n",
        "        img = Image.open(filepath)\n",
        "\n",
        "        plt.subplot(num_rows, num_columns, idx + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Title with key info\n",
        "        title = f\"{metadata['sampler']}\\n{metadata['width']}x{metadata['height']} | {metadata['num_steps']} steps\"\n",
        "        plt.title(title, fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Displayed {num_images} of {len(metadata_list)} images\")\n",
        "elif run_sampler_test:\n",
        "    print(\"ℹ️  Display skipped (sampler test mode - results shown above)\")\n",
        "elif not show_results:\n",
        "    print(\"ℹ️  Display disabled (set show_results = True to enable)\")\n",
        "else:\n",
        "    print(\"ℹ️  No images to display\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkQTr1ptiUNq"
      },
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLEZx4APiUNq"
      },
      "outputs": [],
      "source": [
        "#@markdown Clean up and optionally end session\n",
        "end_session = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Memory cleanup\n",
        "del pipe, compel\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Cleanup complete\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory after cleanup: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")\n",
        "\n",
        "if end_session:\n",
        "    print(\"Ending Colab session...\")\n",
        "    sys.exit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d12cd29326048908d81f9d7452d0af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24de07a45f194cb29b122671009257e8",
              "IPY_MODEL_202d806c538f4079990135bc055f49ea",
              "IPY_MODEL_672401dfae8d4e74aff38bae7a3dc0dd"
            ],
            "layout": "IPY_MODEL_60d551f6a246425bba20472a3b67609a"
          }
        },
        "24de07a45f194cb29b122671009257e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1e96e987706405282d9458e4142298c",
            "placeholder": "​",
            "style": "IPY_MODEL_de7ebfd4c5fe499f83d123eff55aa20a",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "202d806c538f4079990135bc055f49ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99d7ee57082a4dae95349ead6f3c06ca",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80ec41aa290e497397cf4bc51d3e15c8",
            "value": 7
          }
        },
        "672401dfae8d4e74aff38bae7a3dc0dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b526570ff724dc1a88dd2c515f75c4b",
            "placeholder": "​",
            "style": "IPY_MODEL_1c3746593bdd4c8b9de9353416c8646e",
            "value": " 7/7 [00:09&lt;00:00,  1.33s/it]"
          }
        },
        "60d551f6a246425bba20472a3b67609a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1e96e987706405282d9458e4142298c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de7ebfd4c5fe499f83d123eff55aa20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99d7ee57082a4dae95349ead6f3c06ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80ec41aa290e497397cf4bc51d3e15c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b526570ff724dc1a88dd2c515f75c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c3746593bdd4c8b9de9353416c8646e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}