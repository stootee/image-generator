{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgS-fRFOiUNk"
      },
      "source": [
        "# Image Generator with Compel\n",
        "\n",
        "This notebook enhances the basic generator with:\n",
        "- **Compel Integration**: Handle long prompts with weighted embeddings (no 77 token limit)\n",
        "- **Batch Prompt Fetching**: Pre-fetch multiple prompts to reduce API calls\n",
        "- **Overflow Management**: Smart usage of overflow tags from prompt API\n",
        "- **Memory Optimized**: Works on Colab free tier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1jtsQq4iUNm",
        "outputId": "09eb21c0-eb11-4b24-de1f-f60b92f48f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import drive\n",
        "\n",
        "mount_path = '/content/drive'\n",
        "\n",
        "if not os.path.exists(mount_path):\n",
        "    print(\"Drive not mounted. Mounting now...\")\n",
        "    drive.mount(mount_path)\n",
        "else:\n",
        "    print(\"Drive already mounted.\")\n",
        "\n",
        "# Define the path to your file\n",
        "file_path = '/content/drive/MyDrive/AI/hf_token.env'\n",
        "\n",
        "# Load the environment variables\n",
        "load_dotenv(file_path)\n",
        "\n",
        "huggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='Flax classes are deprecated')\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QPxql7TziUNn"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q diffusers transformers accelerate safetensors omegaconf invisible-watermark compel bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK2c7ZnviUNn",
        "outputId": "a8b8cd9c-86a8-4af3-ba3c-172c344cbeaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Initial GPU Memory: 14.64GB free\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "import gc\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import random\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from huggingface_hub import snapshot_download, login\n",
        "from diffusers import (\n",
        "    StableDiffusionXLPipeline,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    KDPM2AncestralDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    UniPCMultistepScheduler\n",
        ")\n",
        "from compel import CompelForSDXL as Compel, ReturnedEmbeddingsType\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Initial GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYIlygeriUNn"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXONf1tJiUNo",
        "outputId": "1c8ef0b4-e030-4cfe-832f-0dddec0902f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Configuration loaded\n",
            "  Model: stablediffusionapi/mklan-xxx-nsfw-pony\n",
            "  Sampler: DPM++ 2M Karras\n",
            "  Guidance: 5-15\n",
            "  Steps: 25-40\n",
            "  Resolution: sdxl\n",
            "  Overflow usage: 100%\n",
            "  Uniform beta schedule: False\n",
            "  Save to: /content/drive/MyDrive/AI/images/20251228112720/\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### Paths and Authentication\n",
        "base_path = \"/content/drive/MyDrive/AI/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Model Configuration\n",
        "model_id = \"stablediffusionapi/mklan-xxx-nsfw-pony\" #@param [\"stablediffusionapi/mklan-xxx-nsfw-pony\",\"stablediffusionapi/duchaiten-real3d-nsfw-xl\", \"John6666/cyberrealistic-pony-v7-sdxl\", \"John6666/uber-realistic-porn-merge-xl-urpmxl-v6final-sdxl\",\"John6666/fucktastic-real-checkpoint-pony-pdxl-porn-realistic-nsfw-sfw-21-sdxl\",\"Vvilams/pony-realism-v21main-sdxl\",\"John6666/sexoholic-real-pony-nsfw-v2-sdxl\",\"John6666/wai-ani-nsfw-ponyxl-v11-sdxl\", \"John6666/duchaiten-pony-real-v20-sdxl\", \"stable-diffusion-v1-5/stable-diffusion-v1-5\", \"stabilityai/stable-diffusion-xl-base-1.0\", \"John6666/pornworks-real-porn-v03-sdxl\", \"UnfilteredAI/NSFW-GEN-ANIME\", \"UnfilteredAI/NSFW-gen-v2\"]\n",
        "download_model = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Prompt Configuration\n",
        "base_prompt = \"\" #@param {type:\"string\"}\n",
        "sfw = False #@param {type:\"boolean\"}\n",
        "selected_categories = [] #@param {type:\"raw\"}\n",
        "prompts_per_batch = 1 #@param {type:\"integer\"}\n",
        "overflow_usage = 1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "#@markdown ### Generation Settings\n",
        "num_of_images = 50 #@param {type:\"integer\"}\n",
        "sampler = \"DPM++ 2M Karras\" #@param [\"Model Default\", \"DPM++ 2M\", \"DPM++ 2M Karras\", \"DPM++ SDE\", \"DPM++ SDE Karras\", \"Euler\", \"Euler a\", \"Heun\", \"KDPM2\", \"KDPM2 a\", \"LMS\", \"DDIM\", \"PNDM\", \"UniPC\"]\n",
        "guidance_low = 5 #@param {type:\"integer\"}\n",
        "guidance_high = 15 #@param {type:\"integer\"}\n",
        "steps_low = 25 #@param {type:\"integer\"}\n",
        "steps_high = 40 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Resolution Settings\n",
        "resolution_mode = \"sdxl\" #@param [\"classic\", \"sdxl\"]\n",
        "\n",
        "# Resolution pools\n",
        "CLASSIC_RESOLUTIONS = [720, 768, 800, 1024]\n",
        "SDXL_RESOLUTIONS = [\n",
        "    (1024, 1024), (1152, 896), (896, 1152), (1216, 832),\n",
        "    (832, 1216), (1344, 768), (768, 1344)\n",
        "]\n",
        "\n",
        "# Derived paths\n",
        "model_path = base_path + \"models/\" + model_id\n",
        "save_directory = f\"{base_path}images/{datetime.now().strftime('%Y%m%d%H%M%S')}/\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# Models requiring uniform beta_schedule instead of scaled_linear\n",
        "uniform_models = [\n",
        "    \"John6666/uber-realistic-porn-merge-xl-urpmxl-v6final-sdxl\",\n",
        "    \"John6666/sexoholic-real-pony-nsfw-v2-sdxl\",\n",
        "    \"John6666/duchaiten-pony-real-v20-sdxl\",\n",
        "    \"stabilityai/sdxl-turbo\",\n",
        "    \"John6666/pornworks-real-porn-v03-sdxl\"\n",
        "]\n",
        "use_uniform = model_id in uniform_models\n",
        "\n",
        "# Fallback negative prompt (API now provides negative_prompt per image)\n",
        "NEGATIVE_PROMPT = \"text, writing, bad teeth, deformed face and eyes, child, childish, young, deformed, uneven eyes, too many fingers\"\n",
        "\n",
        "print(f\"✓ Configuration loaded\")\n",
        "print(f\"  Model: {model_id}\")\n",
        "print(f\"  Sampler: {sampler}\")\n",
        "print(f\"  Guidance: {guidance_low}-{guidance_high}\")\n",
        "print(f\"  Steps: {steps_low}-{steps_high}\")\n",
        "print(f\"  Resolution: {resolution_mode}\")\n",
        "print(f\"  Overflow usage: {overflow_usage*100:.0f}%\")\n",
        "print(f\"  Uniform beta schedule: {use_uniform}\")\n",
        "print(f\"  Save to: {save_directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN5WT1lciUNo"
      },
      "source": [
        "## Prompt Management System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVu--V7KiUNp",
        "outputId": "91206cf7-ae04-4416-9bbf-f4e1b4bfc70c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-fetching 1 prompts...\n",
            "Fetched 1 prompts successfully\n"
          ]
        }
      ],
      "source": [
        "class PromptManager:\n",
        "    \"\"\"Manages prompt fetching, caching, and variation generation.\"\"\"\n",
        "\n",
        "    def __init__(self, api_url_template, cache_size=20):\n",
        "        self.api_url_template = api_url_template\n",
        "        self.cache = []\n",
        "        self.cache_size = cache_size\n",
        "        self.stats = defaultdict(int)\n",
        "\n",
        "    def fetch_prompts(self, count=5):\n",
        "        \"\"\"Fetch multiple prompts at once and cache them.\"\"\"\n",
        "        prompts = []\n",
        "        for _ in range(count):\n",
        "            try:\n",
        "                response = requests.get(self.api_url_template, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    # Parse overflow - API returns comma-separated string, not array\n",
        "                    overflow_raw = data.get('overflow', '')\n",
        "                    if isinstance(overflow_raw, str):\n",
        "                        # Split by comma and strip whitespace\n",
        "                        overflow = [item.strip() for item in overflow_raw.split(',') if item.strip()]\n",
        "                    elif isinstance(overflow_raw, list):\n",
        "                        # Already a list (in case API changes)\n",
        "                        overflow = overflow_raw\n",
        "                    else:\n",
        "                        overflow = []\n",
        "\n",
        "                    prompts.append({\n",
        "                        'prompt': data['prompt'],\n",
        "                        'overflow': overflow,\n",
        "                        'refined': data.get('refined', ''),\n",
        "                        'negative_prompt': data.get('negative_prompt', NEGATIVE_PROMPT)  # Use API's negative or fallback\n",
        "                    })\n",
        "                    self.stats['fetched'] += 1\n",
        "                else:\n",
        "                    print(f\"Failed to fetch prompt: {response.status_code}\")\n",
        "                    self.stats['failed'] += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching prompt: {e}\")\n",
        "                self.stats['failed'] += 1\n",
        "\n",
        "        self.cache.extend(prompts)\n",
        "        # Keep cache size manageable\n",
        "        if len(self.cache) > self.cache_size:\n",
        "            self.cache = self.cache[-self.cache_size:]\n",
        "\n",
        "        return len(prompts)\n",
        "\n",
        "    def get_prompt(self):\n",
        "        \"\"\"Get a prompt from cache or fetch new ones if cache is low.\"\"\"\n",
        "        if len(self.cache) < 3:\n",
        "            print(f\"Cache low ({len(self.cache)} prompts), fetching more...\")\n",
        "            self.fetch_prompts(prompts_per_batch)\n",
        "\n",
        "        if not self.cache:\n",
        "            # Emergency fetch\n",
        "            self.fetch_prompts(1)\n",
        "\n",
        "        if self.cache:\n",
        "            self.stats['used'] += 1\n",
        "            return self.cache.pop(0)\n",
        "        return None\n",
        "\n",
        "    def build_weighted_prompt(self, prompt_data, overflow_usage=0.6):\n",
        "        \"\"\"\n",
        "        Build weighted prompt with smart overflow usage.\n",
        "\n",
        "        Args:\n",
        "            prompt_data: Dict with 'prompt', 'overflow', 'refined', 'negative_prompt'\n",
        "            overflow_usage: Fraction of overflow items to use (0.0-1.0)\n",
        "        \"\"\"\n",
        "        prompt = prompt_data['prompt']\n",
        "        overflow = prompt_data.get('overflow', [])\n",
        "\n",
        "        if not overflow:\n",
        "            return prompt, 0\n",
        "\n",
        "        # Select random subset of overflow\n",
        "        count = max(1, int(len(overflow) * random.uniform(overflow_usage * 0.7, overflow_usage * 1.3)))\n",
        "        count = min(count, len(overflow))\n",
        "        selected = random.sample(overflow, k=count)\n",
        "\n",
        "        # Build final prompt\n",
        "        overflow_text = \", \".join(selected)\n",
        "        final_prompt = f\"{prompt}, {overflow_text}\"\n",
        "\n",
        "        return final_prompt, count\n",
        "\n",
        "    def print_stats(self):\n",
        "        print(f\"\\nPrompt Manager Stats:\")\n",
        "        print(f\"  Fetched: {self.stats['fetched']}\")\n",
        "        print(f\"  Used: {self.stats['used']}\")\n",
        "        print(f\"  Failed: {self.stats['failed']}\")\n",
        "        print(f\"  Cached: {len(self.cache)}\")\n",
        "\n",
        "# Initialize prompt manager\n",
        "prompt_url = f\"https://prompt-gen.squigglypickle.co.uk/generate-prompt?sfw={sfw}&base_prompt={base_prompt}&selected_categories={' '.join(selected_categories)}\"\n",
        "prompt_manager = PromptManager(prompt_url)\n",
        "\n",
        "# Pre-fetch prompts\n",
        "print(f\"Pre-fetching {prompts_per_batch} prompts...\")\n",
        "fetched = prompt_manager.fetch_prompts(prompts_per_batch)\n",
        "print(f\"Fetched {fetched} prompts successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592,
          "referenced_widgets": [
            "bc3b426540d94eacb03cca1883298e90",
            "cb2eecf2917e43ee9811526d17ee5f77",
            "d60a436407254545b342388ed904025e",
            "f0087fad1255413fb43097ca2f989bab",
            "7e7bd6fee20148da947e269c3efd9312",
            "02623ccd32ac40e2abd5f245e169cbc2",
            "8373d94faacf4453bac44123fafc7f94",
            "7c641a3cd57a47c2b5d4a5c6df638f70",
            "0abc84d2bf47472eab63445041233f7a",
            "d255247877d84e1686c72ac87be89518",
            "ae8d505649b04a4e954f3a417ae0df34",
            "211c5aaa92634964abb99bbece74179a",
            "29047875ef224bc495dce9341794f9e2",
            "37533c5e017b4fad84c210483d2a8e69",
            "f1ce9997cbf041d5a21fb04cd215b476",
            "8cc34dc3590c40fca2c253df0b25f56a",
            "7737ad41f6ae4205a5c70b08d8e20784",
            "2f9c2e26eb6c4b6387c9c81850434667",
            "fa7c632a97fe482c92f4e6739bc2373f",
            "a32f8c08800f4ad0b1183086d2039f5c",
            "49c24c696f50474291d8801f374e872e",
            "5d0efb97137a4f4e85480c1574556f99",
            "365b6ba1b27645278481a14448f2773f",
            "595ff59e18cc4933984584b9c8b55e16",
            "2a571908e6b44a0e886eda60b82a9740",
            "fc2bc51109c34822b7a350c7dc1a205a",
            "27fcd3990d51425fbdc48516b4b9e4b9",
            "c967b662288248dd8b70a5b653302789",
            "22828e38201f44a3a4a8a893e053e4d0",
            "78fcacb44dd2420497dbf0e71b36841e",
            "94ccc908d1b340c78a450cda5f7de343",
            "06f61b16928e45eeaa84d9bd3cdd0478",
            "b46304dabe364795a6c7d8e73753c7f0"
          ]
        },
        "id": "8eHhc0oLiUNp",
        "outputId": "1ea62fdd-b462-4605-d0f6-298dde2ac0ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model already exists at /content/drive/MyDrive/AI/models/stablediffusionapi/mklan-xxx-nsfw-pony, skipping download\n",
            "Loading model stablediffusionapi/mklan-xxx-nsfw-pony...\n",
            "Attempting to load with fp16 variant...\n",
            "Retrying without variant (using full precision weights)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc3b426540d94eacb03cca1883298e90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded from local path: /content/drive/MyDrive/AI/models/stablediffusionapi/mklan-xxx-nsfw-pony\n",
            "✓ Model loaded successfully (no_variant)\n",
            "\n",
            "Loading enhanced VAE for better color accuracy...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "211c5aaa92634964abb99bbece74179a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "365b6ba1b27645278481a14448f2773f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Enhanced VAE loaded from HuggingFace\n",
            "✓ Enhanced VAE applied to pipeline\n",
            "\n",
            "Applying memory optimizations...\n",
            "⚠ xformers not available\n",
            "⚠ CPU offload disabled (incompatible with Compel)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "CompelForSDXL.__init__() got an unexpected keyword argument 'tokenizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-962557535.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m# Initialize Compel for advanced prompt weighting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m compel = Compel(\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mtext_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoder_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CompelForSDXL.__init__() got an unexpected keyword argument 'tokenizer'"
          ]
        }
      ],
      "source": [
        "# Check if model needs to be downloaded\n",
        "model_exists = os.path.exists(model_path) and os.path.isdir(model_path) and len(os.listdir(model_path)) > 0\n",
        "should_download = download_model or not model_exists\n",
        "\n",
        "if should_download:\n",
        "    if not model_exists:\n",
        "        print(f\"Model not found at {model_path}, downloading from HuggingFace...\")\n",
        "    elif download_model:\n",
        "        print(f\"Re-downloading model {model_id} (download_model checkbox enabled)...\")\n",
        "\n",
        "    if not huggingface_token:\n",
        "        print(\"Warning: No HuggingFace token provided. Download may fail for gated models.\")\n",
        "\n",
        "    snapshot_download(\n",
        "        repo_id=model_id,\n",
        "        local_dir=model_path,\n",
        "        token=huggingface_token if huggingface_token else None,\n",
        "        ignore_patterns=[\"*.safetensors.lock\"]\n",
        "    )\n",
        "    print(f\"✓ Model downloaded to: {model_path}\")\n",
        "else:\n",
        "    print(f\"✓ Model already exists at {model_path}, skipping download\")\n",
        "\n",
        "# Load pipeline with memory optimizations\n",
        "print(f\"Loading model {model_id}...\")\n",
        "\n",
        "# Base load kwargs\n",
        "load_kwargs = {\n",
        "    \"torch_dtype\": torch.float16,\n",
        "    \"use_safetensors\": True,\n",
        "}\n",
        "\n",
        "# Try loading with fp16 variant first, fallback to no variant if unavailable\n",
        "pipe = None\n",
        "for attempt in [\"fp16\", \"no_variant\"]:\n",
        "    try:\n",
        "        if attempt == \"fp16\":\n",
        "            load_kwargs[\"variant\"] = \"fp16\"\n",
        "            print(\"Attempting to load with fp16 variant...\")\n",
        "        else:\n",
        "            load_kwargs.pop(\"variant\", None)\n",
        "            print(\"Retrying without variant (using full precision weights)...\")\n",
        "\n",
        "        # Try loading from local path first, fall back to HuggingFace if corrupted\n",
        "        try:\n",
        "            if os.path.exists(model_path):\n",
        "                pipe = StableDiffusionXLPipeline.from_pretrained(model_path, **load_kwargs)\n",
        "                print(f\"✓ Loaded from local path: {model_path}\")\n",
        "            else:\n",
        "                raise FileNotFoundError(\"Local model path does not exist\")\n",
        "        except (OSError, FileNotFoundError) as local_error:\n",
        "            # Local model is corrupted or missing, load from HuggingFace\n",
        "            if os.path.exists(model_path):\n",
        "                print(f\"⚠ Local model corrupted: {local_error}\")\n",
        "                print(f\"Loading from HuggingFace instead...\")\n",
        "            load_kwargs[\"token\"] = huggingface_token if huggingface_token else None\n",
        "            pipe = StableDiffusionXLPipeline.from_pretrained(model_id, **load_kwargs)\n",
        "            print(f\"✓ Loaded from HuggingFace: {model_id}\")\n",
        "\n",
        "        print(f\"✓ Model loaded successfully ({attempt})\")\n",
        "        break\n",
        "\n",
        "    except ValueError as e:\n",
        "        if \"variant\" in str(e) and attempt == \"fp16\":\n",
        "            # fp16 variant not available, will retry without variant\n",
        "            continue\n",
        "        else:\n",
        "            # Some other ValueError, re-raise it\n",
        "            raise\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model with {attempt}: {e}\")\n",
        "        if attempt == \"no_variant\":\n",
        "            # Last attempt failed, re-raise\n",
        "            raise\n",
        "\n",
        "if pipe is None:\n",
        "    raise RuntimeError(\"Failed to load model after all attempts\")\n",
        "\n",
        "# NOW load the enhanced VAE (after pipeline is loaded)\n",
        "# This ensures we can properly replace the model's default VAE\n",
        "print(\"\\nLoading enhanced VAE for better color accuracy...\")\n",
        "from diffusers import AutoencoderKL\n",
        "\n",
        "try:\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        \"madebyollin/sdxl-vae-fp16-fix\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    print(\"✓ Enhanced VAE loaded from HuggingFace\")\n",
        "\n",
        "    # Replace the pipeline's VAE with the enhanced one\n",
        "    pipe.vae = vae\n",
        "    print(\"✓ Enhanced VAE applied to pipeline\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Could not load enhanced VAE, using model's default: {e}\")\n",
        "\n",
        "# Move pipeline to GPU\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# Memory optimizations\n",
        "print(\"\\nApplying memory optimizations...\")\n",
        "pipe.enable_attention_slicing()\n",
        "pipe.enable_vae_slicing()\n",
        "pipe.enable_vae_tiling()\n",
        "\n",
        "# Try to enable xformers for better memory efficiency\n",
        "try:\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "    print(\"✓ xformers enabled\")\n",
        "except:\n",
        "    print(\"⚠ xformers not available\")\n",
        "\n",
        "# NOTE: enable_model_cpu_offload() is DISABLED because it conflicts with Compel\n",
        "# Compel needs text encoders to stay on GPU for prompt weighting\n",
        "print(\"⚠ CPU offload disabled (incompatible with Compel)\")\n",
        "\n",
        "# Initialize Compel for advanced prompt weighting\n",
        "compel = Compel(\n",
        "    tokenizer=[pipe.tokenizer, pipe.tokenizer_2],\n",
        "    text_encoder=[pipe.text_encoder, pipe.text_encoder_2],\n",
        "    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "    requires_pooled=[False, True],\n",
        "    device=\"cuda\"\n",
        ")\n",
        "print(\"✓ Compel initialized for advanced prompt weighting\")\n",
        "\n",
        "print(f\"\\n✓ Pipeline ready!\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGXveRB4iUNp"
      },
      "source": [
        "# Sampler Configuration (simplified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_7mbhnjiUNp"
      },
      "outputs": [],
      "source": [
        "# Sampler classes\n",
        "from diffusers import (\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DPMSolverSinglestepScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    KDPM2DiscreteScheduler,\n",
        "    KDPM2AncestralDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    DDIMScheduler,\n",
        "    PNDMScheduler,\n",
        "    UniPCMultistepScheduler\n",
        ")\n",
        "\n",
        "# Apply uniform beta_schedule for models that require it\n",
        "if use_uniform:\n",
        "    print(f\"Applying uniform beta_schedule for {model_id}...\")\n",
        "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "    pipe.scheduler.config.beta_schedule = \"uniform\"\n",
        "    print(f\"✓ Scheduler set to DPMSolverMultistepScheduler with uniform beta_schedule\")\n",
        "\n",
        "# IMPORTANT: Initialize samplers AFTER pipeline is loaded so we can use the model's scheduler config\n",
        "# This preserves the model's trained scheduler settings (beta schedules, timestep spacing, etc.)\n",
        "\n",
        "def initialize_samplers(base_scheduler_config):\n",
        "    \"\"\"\n",
        "    Create scheduler instances using the model's trained config.\n",
        "    This ensures we inherit important settings like beta_schedule, timestep_spacing, etc.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        # DPM++ variants (multistep)\n",
        "        \"DPM++ 2M\": lambda: DPMSolverMultistepScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"DPM++ 2M Karras\": lambda: DPMSolverMultistepScheduler.from_config(\n",
        "            base_scheduler_config,\n",
        "            use_karras_sigmas=True\n",
        "        ),\n",
        "\n",
        "        # DPM++ SDE variants (singlestep - stochastic)\n",
        "        \"DPM++ SDE\": lambda: DPMSolverSinglestepScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"DPM++ SDE Karras\": lambda: DPMSolverSinglestepScheduler.from_config(\n",
        "            base_scheduler_config,\n",
        "            use_karras_sigmas=True\n",
        "        ),\n",
        "\n",
        "        # Euler variants\n",
        "        \"Euler\": lambda: EulerDiscreteScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"Euler a\": lambda: EulerAncestralDiscreteScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "\n",
        "        # KDPM2 variants\n",
        "        \"KDPM2\": lambda: KDPM2DiscreteScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"KDPM2 a\": lambda: KDPM2AncestralDiscreteScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "\n",
        "        # Other popular schedulers\n",
        "        \"Heun\": lambda: HeunDiscreteScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"LMS\": lambda: LMSDiscreteScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"DDIM\": lambda: DDIMScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"PNDM\": lambda: PNDMScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "        \"UniPC\": lambda: UniPCMultistepScheduler.from_config(\n",
        "            base_scheduler_config\n",
        "        ),\n",
        "\n",
        "        # Model's original scheduler\n",
        "        \"Model Default\": lambda: pipe.scheduler.__class__.from_config(\n",
        "            base_scheduler_config\n",
        "        )\n",
        "    }\n",
        "\n",
        "# Initialize samplers with the model's scheduler config\n",
        "SAMPLERS = initialize_samplers(pipe.scheduler.config)\n",
        "\n",
        "def get_resolution():\n",
        "    \"\"\"Get random resolution based on mode.\"\"\"\n",
        "    if resolution_mode == \"classic\":\n",
        "        width = random.choice(CLASSIC_RESOLUTIONS)\n",
        "        height = random.choice(CLASSIC_RESOLUTIONS)\n",
        "        return (width, height)\n",
        "    else:  # sdxl\n",
        "        return random.choice(SDXL_RESOLUTIONS)\n",
        "\n",
        "print(f\"✓ Samplers configured using model's scheduler config\")\n",
        "print(f\"  Base scheduler: {pipe.scheduler.__class__.__name__}\")\n",
        "print(f\"  Available samplers: {len(SAMPLERS)}\")\n",
        "print(f\"  Config: beta_schedule={pipe.scheduler.config.get('beta_schedule', 'N/A')}, \"\n",
        "      f\"timestep_spacing={pipe.scheduler.config.get('timestep_spacing', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scheduler Comparison Test (Optional)\n",
        "\n",
        "Test all schedulers with identical parameters to compare quality and characteristics."
      ],
      "metadata": {
        "id": "t0AwYJSgjHVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Scheduler Comparison Settings\n",
        "run_sampler_test = True #@param {type:\"boolean\"}\n",
        "test_prompt = \"futa futanari big dick. huge tits, wet pussy. photorealistic, high quality, detailed\" #@param {type:\"string\"}\n",
        "test_seed = 1024 #@param {type:\"integer\"}\n",
        "test_width = 1024 #@param {type:\"integer\"}\n",
        "test_height = 1024 #@param {type:\"integer\"}\n",
        "test_steps = 30 #@param {type:\"integer\"}\n",
        "test_guidance = 7.5 #@param {type:\"number\"}\n",
        "test_columns = 3 #@param {type:\"integer\"}\n",
        "\n",
        "if run_sampler_test:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"SCHEDULER COMPARISON TEST\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nTest Parameters:\")\n",
        "    print(f\"  Prompt: {test_prompt[:60]}...\")\n",
        "    print(f\"  Seed: {test_seed}\")\n",
        "    print(f\"  Resolution: {test_width}x{test_height}\")\n",
        "    print(f\"  Steps: {test_steps}\")\n",
        "    print(f\"  Guidance: {test_guidance}\")\n",
        "    print(f\"\\nGenerating with {len(SAMPLERS)} schedulers...\")\n",
        "\n",
        "    # Create test directory\n",
        "    test_directory = f\"{base_path}scheduler_tests/{datetime.now().strftime('%Y%m%d%H%M%S')}/\"\n",
        "    os.makedirs(test_directory, exist_ok=True)\n",
        "\n",
        "    # Encode prompt once (use for all schedulers)\n",
        "    print(\"\\nEncoding prompt...\")\n",
        "    conditioning, pooled = compel(test_prompt)\n",
        "\n",
        "    # Generate with each scheduler\n",
        "    test_results = []\n",
        "    for scheduler_name in sorted(SAMPLERS.keys()):\n",
        "        try:\n",
        "            print(f\"\\n[{len(test_results)+1}/{len(SAMPLERS)}] Testing: {scheduler_name}\")\n",
        "\n",
        "            # Set scheduler\n",
        "            pipe.scheduler = SAMPLERS[scheduler_name]()\n",
        "\n",
        "            # Generate image\n",
        "            result = pipe(\n",
        "                prompt_embeds=conditioning,\n",
        "                pooled_prompt_embeds=pooled,\n",
        "                negative_prompt=NEGATIVE_PROMPT,\n",
        "                num_images_per_prompt=1,\n",
        "                width=test_width,\n",
        "                height=test_height,\n",
        "                guidance_scale=test_guidance,\n",
        "                num_inference_steps=test_steps,\n",
        "                generator=torch.Generator(device=\"cuda\").manual_seed(test_seed),\n",
        "            ).images[0]\n",
        "\n",
        "            # Save image\n",
        "            filename = f\"{scheduler_name.replace(' ', '_').replace('+', 'p')}.png\"\n",
        "            filepath = os.path.join(test_directory, filename)\n",
        "            result.save(filepath)\n",
        "\n",
        "            test_results.append({\n",
        "                'scheduler': scheduler_name,\n",
        "                'image': result,\n",
        "                'filename': filename\n",
        "            })\n",
        "\n",
        "            print(f\"  ✓ Saved: {filename}\")\n",
        "\n",
        "            # Cleanup\n",
        "            del result\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Cleanup encoded prompt\n",
        "    del conditioning, pooled\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Display results in grid\n",
        "    if test_results:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"RESULTS: {len(test_results)} schedulers tested\")\n",
        "        print(f\"Saved to: {test_directory}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        # Create comparison grid\n",
        "        num_results = len(test_results)\n",
        "        num_rows = (num_results + test_columns - 1) // test_columns\n",
        "\n",
        "        #fig = plt.figure(figsize=(6 * test_columns, 6 * num_rows))\n",
        "\n",
        "        #for idx, result in enumerate(test_results):\n",
        "        #    ax = plt.subplot(num_rows, test_columns, idx + 1)\n",
        "        #    ax.imshow(result['image'])\n",
        "        #    ax.axis('off')\n",
        "        #    ax.set_title(result['scheduler'], fontsize=12, weight='bold')\n",
        "\n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            'prompt': test_prompt,\n",
        "            'negative_prompt': NEGATIVE_PROMPT,\n",
        "            'seed': test_seed,\n",
        "            'width': test_width,\n",
        "            'height': test_height,\n",
        "            'steps': test_steps,\n",
        "            'guidance_scale': test_guidance,\n",
        "            'model': model_id,\n",
        "            'schedulers_tested': [r['scheduler'] for r in test_results],\n",
        "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(test_directory, 'test_metadata.json'), 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "\n",
        "        print(f\"\\nℹ️  To compare specific schedulers, look at the images in:\")\n",
        "        print(f\"   {test_directory}\")\n",
        "    else:\n",
        "        print(\"\\n✗ No results generated\")\n",
        "else:\n",
        "    print(\"ℹ️  Scheduler comparison test disabled\")\n",
        "    print(\"   Set run_sampler_test = True to compare all schedulers with identical parameters\")"
      ],
      "metadata": {
        "id": "D2sBvHpojHVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y162nF67iUNq"
      },
      "source": [
        "# Image Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqu6Jz-7iUNq"
      },
      "outputs": [],
      "source": [
        "metadata_list = []\n",
        "\n",
        "# Setup sampler with clean initialization\n",
        "print(f\"Configuring sampler: {sampler}\")\n",
        "pipe.scheduler = SAMPLERS[sampler]()\n",
        "print(f\"✓ Scheduler initialized: {pipe.scheduler.__class__.__name__}\")\n",
        "\n",
        "print(f\"\\nStarting generation of {num_of_images} images...\")\n",
        "print(f\"Save directory: {save_directory}\\n\")\n",
        "\n",
        "for i in range(num_of_images):\n",
        "    try:\n",
        "        # Get prompt from manager\n",
        "        prompt_data = prompt_manager.get_prompt()\n",
        "        if not prompt_data:\n",
        "            print(f\"Failed to get prompt for image {i}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Build final prompt with overflow\n",
        "        final_prompt, overflow_count = prompt_manager.build_weighted_prompt(\n",
        "            prompt_data, overflow_usage\n",
        "        )\n",
        "\n",
        "        # Randomize parameters\n",
        "        seed = random.randint(0, 2**32 - 1)\n",
        "        width, height = get_resolution()\n",
        "        guidance_scale = round(random.uniform(guidance_low, guidance_high) * 2) / 2\n",
        "        num_steps = random.randint(steps_low, steps_high)\n",
        "\n",
        "        # Use Compel for prompt encoding (handles long prompts)\n",
        "        conditioning, pooled = compel(final_prompt)\n",
        "\n",
        "        # Generate image\n",
        "        result = pipe(\n",
        "            prompt_embeds=conditioning,\n",
        "            pooled_prompt_embeds=pooled,\n",
        "            negative_prompt=prompt_data.get('negative_prompt', NEGATIVE_PROMPT),  # Use API's negative prompt\n",
        "            num_images_per_prompt=1,\n",
        "            width=width,\n",
        "            height=height,\n",
        "            guidance_scale=guidance_scale,\n",
        "            num_inference_steps=num_steps,\n",
        "            generator=torch.Generator(device=\"cuda\").manual_seed(seed),\n",
        "        ).images[0]\n",
        "\n",
        "        # Save image\n",
        "        filename = f\"{datetime.now().strftime('%Y%m%d%H%M%S')}_{str(i).zfill(3)}.png\"\n",
        "        filepath = os.path.join(save_directory, filename)\n",
        "        result.save(filepath)\n",
        "\n",
        "        # Store metadata\n",
        "        metadata = {\n",
        "            \"filename\": filename,\n",
        "            \"model\": model_id,\n",
        "            \"sampler\": sampler,\n",
        "            \"base_prompt\": base_prompt,\n",
        "            \"prompt\": prompt_data['prompt'],\n",
        "            \"final_prompt\": final_prompt,\n",
        "            \"overflow_total\": len(prompt_data.get('overflow', [])),\n",
        "            \"overflow_used\": overflow_count,\n",
        "            \"negative_prompt\": prompt_data.get('negative_prompt', NEGATIVE_PROMPT),\n",
        "            \"sfw\": sfw,\n",
        "            \"seed\": seed,\n",
        "            \"width\": width,\n",
        "            \"height\": height,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_steps\": num_steps\n",
        "        }\n",
        "        metadata_list.append(metadata)\n",
        "\n",
        "        # Progress update\n",
        "        overflow_info = f\" +{overflow_count} overflow\" if overflow_count > 0 else \"\"\n",
        "        print(f\"[{i+1}/{num_of_images}] {width}x{height} | {num_steps} steps | G:{guidance_scale:.1f}{overflow_info}\")\n",
        "\n",
        "        # Memory cleanup\n",
        "        del result, conditioning, pooled\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image {i}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        continue\n",
        "\n",
        "# Save metadata\n",
        "metadata_path = os.path.join(save_directory, \"metadata.json\")\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata_list, f, indent=2)\n",
        "\n",
        "print(f\"\\n✓ Generation complete!\")\n",
        "print(f\"✓ {len(metadata_list)} images saved to: {save_directory}\")\n",
        "print(f\"✓ Metadata saved to: {metadata_path}\")\n",
        "\n",
        "# Print stats\n",
        "prompt_manager.print_stats()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nFinal GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0fTFpZxiUNq"
      },
      "source": [
        "## Statistics and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAd9ct_iiUNq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "if not run_sampler_test and 'metadata_list' in locals() and metadata_list:\n",
        "    # Sampler distribution\n",
        "    samplers = [m['sampler'] for m in metadata_list]\n",
        "    sampler_counts = Counter(samplers)\n",
        "\n",
        "    # Resolution distribution\n",
        "    resolutions = [f\"{m['width']}x{m['height']}\" for m in metadata_list]\n",
        "    resolution_counts = Counter(resolutions)\n",
        "\n",
        "    # Overflow usage\n",
        "    avg_overflow_used = sum(m['overflow_used'] for m in metadata_list) / len(metadata_list)\n",
        "    avg_overflow_total = sum(m['overflow_total'] for m in metadata_list) / len(metadata_list)\n",
        "\n",
        "    print(\"\\n=== Generation Statistics ===\")\n",
        "    print(f\"\\nSampler Usage:\")\n",
        "    for sampler, count in sampler_counts.most_common():\n",
        "        print(f\"  {sampler}: {count} ({count/len(metadata_list)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nTop 5 Resolutions:\")\n",
        "    for res, count in resolution_counts.most_common(5):\n",
        "        print(f\"  {res}: {count} ({count/len(metadata_list)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nOverflow Usage:\")\n",
        "    print(f\"  Average used: {avg_overflow_used:.1f}\")\n",
        "    print(f\"  Average total: {avg_overflow_total:.1f}\")\n",
        "    print(f\"  Usage rate: {avg_overflow_used/avg_overflow_total*100:.1f}%\" if avg_overflow_total > 0 else \"  N/A\")\n",
        "elif run_sampler_test:\n",
        "    print(\"ℹ️  Statistics skipped (sampler test mode - see test results above)\")\n",
        "else:\n",
        "    print(\"ℹ️  No metadata available for statistics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V9JGiQwiUNq"
      },
      "source": [
        "## Display Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vCC94IoiUNq"
      },
      "outputs": [],
      "source": [
        "#@markdown Display generated images in a grid\n",
        "show_results = True #@param {type:\"boolean\"}\n",
        "num_columns = 3 #@param {type:\"integer\"}\n",
        "max_images_to_show = 12 #@param {type:\"integer\"}\n",
        "\n",
        "if not run_sampler_test and show_results and 'metadata_list' in locals() and metadata_list:\n",
        "    display_metadata = metadata_list[:max_images_to_show]\n",
        "    num_images = len(display_metadata)\n",
        "    num_rows = (num_images + num_columns - 1) // num_columns\n",
        "\n",
        "    plt.figure(figsize=(5 * num_columns, 5 * num_rows))\n",
        "\n",
        "    for idx, metadata in enumerate(display_metadata):\n",
        "        filepath = os.path.join(save_directory, metadata['filename'])\n",
        "        img = Image.open(filepath)\n",
        "\n",
        "        plt.subplot(num_rows, num_columns, idx + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Title with key info\n",
        "        title = f\"{metadata['sampler']}\\n{metadata['width']}x{metadata['height']} | {metadata['num_steps']} steps\"\n",
        "        plt.title(title, fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Displayed {num_images} of {len(metadata_list)} images\")\n",
        "elif run_sampler_test:\n",
        "    print(\"ℹ️  Display skipped (sampler test mode - results shown above)\")\n",
        "elif not show_results:\n",
        "    print(\"ℹ️  Display disabled (set show_results = True to enable)\")\n",
        "else:\n",
        "    print(\"ℹ️  No images to display\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkQTr1ptiUNq"
      },
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLEZx4APiUNq"
      },
      "outputs": [],
      "source": [
        "#@markdown Clean up and optionally end session\n",
        "end_session = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Memory cleanup\n",
        "del pipe, compel\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Cleanup complete\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory after cleanup: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")\n",
        "\n",
        "if end_session:\n",
        "    print(\"Ending Colab session...\")\n",
        "    sys.exit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bc3b426540d94eacb03cca1883298e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb2eecf2917e43ee9811526d17ee5f77",
              "IPY_MODEL_d60a436407254545b342388ed904025e",
              "IPY_MODEL_f0087fad1255413fb43097ca2f989bab"
            ],
            "layout": "IPY_MODEL_7e7bd6fee20148da947e269c3efd9312"
          }
        },
        "cb2eecf2917e43ee9811526d17ee5f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02623ccd32ac40e2abd5f245e169cbc2",
            "placeholder": "​",
            "style": "IPY_MODEL_8373d94faacf4453bac44123fafc7f94",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "d60a436407254545b342388ed904025e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c641a3cd57a47c2b5d4a5c6df638f70",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0abc84d2bf47472eab63445041233f7a",
            "value": 7
          }
        },
        "f0087fad1255413fb43097ca2f989bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d255247877d84e1686c72ac87be89518",
            "placeholder": "​",
            "style": "IPY_MODEL_ae8d505649b04a4e954f3a417ae0df34",
            "value": " 7/7 [02:23&lt;00:00, 13.29s/it]"
          }
        },
        "7e7bd6fee20148da947e269c3efd9312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02623ccd32ac40e2abd5f245e169cbc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8373d94faacf4453bac44123fafc7f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c641a3cd57a47c2b5d4a5c6df638f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0abc84d2bf47472eab63445041233f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d255247877d84e1686c72ac87be89518": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae8d505649b04a4e954f3a417ae0df34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "211c5aaa92634964abb99bbece74179a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29047875ef224bc495dce9341794f9e2",
              "IPY_MODEL_37533c5e017b4fad84c210483d2a8e69",
              "IPY_MODEL_f1ce9997cbf041d5a21fb04cd215b476"
            ],
            "layout": "IPY_MODEL_8cc34dc3590c40fca2c253df0b25f56a"
          }
        },
        "29047875ef224bc495dce9341794f9e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7737ad41f6ae4205a5c70b08d8e20784",
            "placeholder": "​",
            "style": "IPY_MODEL_2f9c2e26eb6c4b6387c9c81850434667",
            "value": "config.json: 100%"
          }
        },
        "37533c5e017b4fad84c210483d2a8e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa7c632a97fe482c92f4e6739bc2373f",
            "max": 631,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a32f8c08800f4ad0b1183086d2039f5c",
            "value": 631
          }
        },
        "f1ce9997cbf041d5a21fb04cd215b476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49c24c696f50474291d8801f374e872e",
            "placeholder": "​",
            "style": "IPY_MODEL_5d0efb97137a4f4e85480c1574556f99",
            "value": " 631/631 [00:00&lt;00:00, 6.75kB/s]"
          }
        },
        "8cc34dc3590c40fca2c253df0b25f56a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7737ad41f6ae4205a5c70b08d8e20784": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f9c2e26eb6c4b6387c9c81850434667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa7c632a97fe482c92f4e6739bc2373f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32f8c08800f4ad0b1183086d2039f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49c24c696f50474291d8801f374e872e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d0efb97137a4f4e85480c1574556f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "365b6ba1b27645278481a14448f2773f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_595ff59e18cc4933984584b9c8b55e16",
              "IPY_MODEL_2a571908e6b44a0e886eda60b82a9740",
              "IPY_MODEL_fc2bc51109c34822b7a350c7dc1a205a"
            ],
            "layout": "IPY_MODEL_27fcd3990d51425fbdc48516b4b9e4b9"
          }
        },
        "595ff59e18cc4933984584b9c8b55e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c967b662288248dd8b70a5b653302789",
            "placeholder": "​",
            "style": "IPY_MODEL_22828e38201f44a3a4a8a893e053e4d0",
            "value": "diffusion_pytorch_model.safetensors: 100%"
          }
        },
        "2a571908e6b44a0e886eda60b82a9740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78fcacb44dd2420497dbf0e71b36841e",
            "max": 334643238,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94ccc908d1b340c78a450cda5f7de343",
            "value": 334643238
          }
        },
        "fc2bc51109c34822b7a350c7dc1a205a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06f61b16928e45eeaa84d9bd3cdd0478",
            "placeholder": "​",
            "style": "IPY_MODEL_b46304dabe364795a6c7d8e73753c7f0",
            "value": " 335M/335M [00:03&lt;00:00, 227MB/s]"
          }
        },
        "27fcd3990d51425fbdc48516b4b9e4b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c967b662288248dd8b70a5b653302789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22828e38201f44a3a4a8a893e053e4d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78fcacb44dd2420497dbf0e71b36841e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ccc908d1b340c78a450cda5f7de343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06f61b16928e45eeaa84d9bd3cdd0478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b46304dabe364795a6c7d8e73753c7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}