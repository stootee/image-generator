{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgS-fRFOiUNk"
      },
      "source": [
        "# Image Generator with Compel\n",
        "\n",
        "This notebook enhances the basic generator with:\n",
        "- **Compel Integration**: Handle long prompts with weighted embeddings (no 77 token limit)\n",
        "- **Batch Prompt Fetching**: Pre-fetch multiple prompts to reduce API calls\n",
        "- **Overflow Management**: Smart usage of overflow tags from prompt API\n",
        "- **Memory Optimized**: Works on Colab free tier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r1jtsQq4iUNm",
        "outputId": "3f78f9d8-b018-494d-e195-1e9c0c486478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mount_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2935308614.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmount_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Drive not mounted. Mounting now...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mount_path' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import drive\n",
        "\n",
        "mount_path = '/content/drive'\n",
        "\n",
        "if not os.path.exists(mount_path):\n",
        "    print(\"Drive not mounted. Mounting now...\")\n",
        "    drive.mount(mount_path)\n",
        "else:\n",
        "    print(\"Drive already mounted.\")\n",
        "\n",
        "# Define the path to your file\n",
        "file_path = '/content/drive/MyDrive/hf_token.env'\n",
        "\n",
        "# Load the environment variables\n",
        "load_dotenv(file_path)\n",
        "\n",
        "# Access the variable (replace 'HF_TOKEN' with the actual key name in your file)\n",
        "hf_token = os.getenv('HF_TOKEN')\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='Flax classes are deprecated')\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPxql7TziUNn"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q diffusers transformers accelerate safetensors omegaconf invisible-watermark compel bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK2c7ZnviUNn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "import gc\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import random\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from huggingface_hub import snapshot_download, login\n",
        "from diffusers import (\n",
        "    StableDiffusionXLPipeline,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    KDPM2AncestralDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    UniPCMultistepScheduler\n",
        ")\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Initial GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYIlygeriUNn"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0Q8t6IniUNo"
      },
      "source": [
        "# Sampler selection removed - use settings in next cell instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1GbFn5viUNo"
      },
      "outputs": [],
      "source": [
        "# This cell has been removed - sampler selection now in main config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXONf1tJiUNo"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Paths and Authentication\n",
        "base_path = \"/content/drive/MyDrive/AI/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Model Configuration\n",
        "model_id = \"Vvilams/pony-realism-v21main-sdxl\" #@param [\"stablediffusionapi/mklan-xxx-nsfw-pony\",\"stablediffusionapi/duchaiten-real3d-nsfw-xl\", \"John6666/cyberrealistic-pony-v7-sdxl\", \"John6666/uber-realistic-porn-merge-xl-urpmxl-v6final-sdxl\",\"John6666/fucktastic-real-checkpoint-pony-pdxl-porn-realistic-nsfw-sfw-21-sdxl\",\"Vvilams/pony-realism-v21main-sdxl\",\"John6666/sexoholic-real-pony-nsfw-v2-sdxl\",\"John6666/wai-ani-nsfw-ponyxl-v11-sdxl\", \"John6666/duchaiten-pony-real-v20-sdxl\", \"stable-diffusion-v1-5/stable-diffusion-v1-5\", \"stabilityai/stable-diffusion-xl-base-1.0\", \"John6666/pornworks-real-porn-v03-sdxl\", \"UnfilteredAI/NSFW-GEN-ANIME\", \"UnfilteredAI/NSFW-gen-v2\"]\n",
        "download_model = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Prompt Configuration\n",
        "base_prompt = \"\" #@param {type:\"string\"}\n",
        "sfw = False #@param {type:\"boolean\"}\n",
        "selected_categories = [] #@param {type:\"raw\"}\n",
        "prompts_per_batch = 5 #@param {type:\"integer\"}\n",
        "overflow_usage = 0.6 #@param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "#@markdown ### Generation Settings\n",
        "num_of_images = 50 #@param {type:\"integer\"}\n",
        "sampler = \"DPM++ 2M Karras\" #@param [\"DPM++ 2M\", \"DPM++ 2M Karras\", \"Euler a\", \"Heun\", \"KDPM2 a\", \"UniPC\"]\n",
        "guidance_low = 5 #@param {type:\"integer\"}\n",
        "guidance_high = 15 #@param {type:\"integer\"}\n",
        "steps_low = 25 #@param {type:\"integer\"}\n",
        "steps_high = 40 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Resolution Settings\n",
        "resolution_mode = \"classic\" #@param [\"classic\", \"sdxl\"]\n",
        "\n",
        "# Resolution pools\n",
        "CLASSIC_RESOLUTIONS = [720, 768, 800, 1024]\n",
        "SDXL_RESOLUTIONS = [\n",
        "    (1024, 1024), (1152, 896), (896, 1152), (1216, 832),\n",
        "    (832, 1216), (1344, 768), (768, 1344)\n",
        "]\n",
        "\n",
        "# Derived paths\n",
        "model_path = base_path + \"models/\" + model_id\n",
        "save_directory = f\"{base_path}images/{datetime.now().strftime('%Y%m%d%H%M%S')}/\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# Models requiring uniform scheduler\n",
        "uniform_models = [\n",
        "    \"John6666/uber-realistic-porn-merge-xl-urpmxl-v6final-sdxl\",\n",
        "    \"John6666/sexoholic-real-pony-nsfw-v2-sdxl\",\n",
        "    \"John6666/duchaiten-pony-real-v20-sdxl\",\n",
        "    \"stabilityai/sdxl-turbo\",\n",
        "    \"John6666/pornworks-real-porn-v03-sdxl\"\n",
        "]\n",
        "use_uniform = model_id in uniform_models\n",
        "\n",
        "# Negative prompt (simple and proven)\n",
        "NEGATIVE_PROMPT = \"text, writing, bad teeth, deformed face and eyes, child, childish, young, deformed, uneven eyes, too many fingers\"\n",
        "\n",
        "print(f\"✓ Configuration loaded\")\n",
        "print(f\"  Model: {model_id}\")\n",
        "print(f\"  Sampler: {sampler}\")\n",
        "print(f\"  Guidance: {guidance_low}-{guidance_high}\")\n",
        "print(f\"  Steps: {steps_low}-{steps_high}\")\n",
        "print(f\"  Resolution: {resolution_mode}\")\n",
        "print(f\"  Overflow usage: {overflow_usage*100:.0f}%\")\n",
        "print(f\"  Save to: {save_directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN5WT1lciUNo"
      },
      "source": [
        "## Prompt Management System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVu--V7KiUNp"
      },
      "outputs": [],
      "source": [
        "class PromptManager:\n",
        "    \"\"\"Manages prompt fetching, caching, and variation generation.\"\"\"\n",
        "\n",
        "    def __init__(self, api_url_template, cache_size=20):\n",
        "        self.api_url_template = api_url_template\n",
        "        self.cache = []\n",
        "        self.cache_size = cache_size\n",
        "        self.stats = defaultdict(int)\n",
        "\n",
        "    def fetch_prompts(self, count=5):\n",
        "        \"\"\"Fetch multiple prompts at once and cache them.\"\"\"\n",
        "        prompts = []\n",
        "        for _ in range(count):\n",
        "            try:\n",
        "                response = requests.get(self.api_url_template, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    # Parse overflow - API returns comma-separated string, not array\n",
        "                    overflow_raw = data.get('overflow', '')\n",
        "                    if isinstance(overflow_raw, str):\n",
        "                        # Split by comma and strip whitespace\n",
        "                        overflow = [item.strip() for item in overflow_raw.split(',') if item.strip()]\n",
        "                    elif isinstance(overflow_raw, list):\n",
        "                        # Already a list (in case API changes)\n",
        "                        overflow = overflow_raw\n",
        "                    else:\n",
        "                        overflow = []\n",
        "\n",
        "                    prompts.append({\n",
        "                        'prompt': data['prompt'],\n",
        "                        'overflow': overflow,\n",
        "                        'refined': data.get('refined', '')\n",
        "                    })\n",
        "                    self.stats['fetched'] += 1\n",
        "                else:\n",
        "                    print(f\"Failed to fetch prompt: {response.status_code}\")\n",
        "                    self.stats['failed'] += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching prompt: {e}\")\n",
        "                self.stats['failed'] += 1\n",
        "\n",
        "        self.cache.extend(prompts)\n",
        "        # Keep cache size manageable\n",
        "        if len(self.cache) > self.cache_size:\n",
        "            self.cache = self.cache[-self.cache_size:]\n",
        "\n",
        "        return len(prompts)\n",
        "\n",
        "    def get_prompt(self):\n",
        "        \"\"\"Get a prompt from cache or fetch new ones if cache is low.\"\"\"\n",
        "        if len(self.cache) < 3:\n",
        "            print(f\"Cache low ({len(self.cache)} prompts), fetching more...\")\n",
        "            self.fetch_prompts(prompts_per_batch)\n",
        "\n",
        "        if not self.cache:\n",
        "            # Emergency fetch\n",
        "            self.fetch_prompts(1)\n",
        "\n",
        "        if self.cache:\n",
        "            self.stats['used'] += 1\n",
        "            return self.cache.pop(0)\n",
        "        return None\n",
        "\n",
        "    def build_weighted_prompt(self, prompt_data, overflow_usage=0.6):\n",
        "        \"\"\"\n",
        "        Build weighted prompt with smart overflow usage.\n",
        "\n",
        "        Args:\n",
        "            prompt_data: Dict with 'prompt', 'overflow', 'refined'\n",
        "            overflow_usage: Fraction of overflow items to use (0.0-1.0)\n",
        "        \"\"\"\n",
        "        prompt = prompt_data['prompt']\n",
        "        overflow = prompt_data.get('overflow', [])\n",
        "\n",
        "        if not overflow:\n",
        "            return prompt, 0\n",
        "\n",
        "        # Select random subset of overflow\n",
        "        count = max(1, int(len(overflow) * random.uniform(overflow_usage * 0.7, overflow_usage * 1.3)))\n",
        "        count = min(count, len(overflow))\n",
        "        selected = random.sample(overflow, k=count)\n",
        "\n",
        "        # Build final prompt\n",
        "        overflow_text = \", \".join(selected)\n",
        "        final_prompt = f\"{prompt}, {overflow_text}\"\n",
        "\n",
        "        return final_prompt, count\n",
        "\n",
        "    def print_stats(self):\n",
        "        print(f\"\\nPrompt Manager Stats:\")\n",
        "        print(f\"  Fetched: {self.stats['fetched']}\")\n",
        "        print(f\"  Used: {self.stats['used']}\")\n",
        "        print(f\"  Failed: {self.stats['failed']}\")\n",
        "        print(f\"  Cached: {len(self.cache)}\")\n",
        "\n",
        "# Initialize prompt manager\n",
        "prompt_url = f\"https://prompt-gen.squigglypickle.co.uk/generate-prompt?sfw={sfw}&base_prompt={base_prompt}&selected_categories={' '.join(selected_categories)}\"\n",
        "prompt_manager = PromptManager(prompt_url)\n",
        "\n",
        "# Pre-fetch prompts\n",
        "print(f\"Pre-fetching {prompts_per_batch} prompts...\")\n",
        "fetched = prompt_manager.fetch_prompts(prompts_per_batch)\n",
        "print(f\"Fetched {fetched} prompts successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUs2EDXfiUNp"
      },
      "source": [
        "## Advanced Negative Prompt Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQeoNvt_iUNp"
      },
      "outputs": [],
      "source": [
        "def generate_dynamic_negative(prompt, sfw=False):\n",
        "    \"\"\"\n",
        "    Generate context-aware negative prompts.\n",
        "    Adapts to content type and randomizes quality negatives.\n",
        "    \"\"\"\n",
        "    # Core quality negatives (always included)\n",
        "    base = \"low quality, worst quality, normal quality, blurry, hazy, out of focus\"\n",
        "\n",
        "    # Detect style from prompt\n",
        "    prompt_lower = prompt.lower()\n",
        "    is_photo = any(kw in prompt_lower for kw in [\"photo\", \"photograph\", \"realistic\", \"photorealistic\"])\n",
        "    is_artistic = any(kw in prompt_lower for kw in [\"painting\", \"drawing\", \"sketch\", \"anime\", \"cartoon\", \"illustration\"])\n",
        "    is_portrait = any(kw in prompt_lower for kw in [\"face\", \"portrait\", \"person\", \"woman\", \"man\", \"girl\", \"boy\"])\n",
        "\n",
        "    # Style-specific negatives\n",
        "    if is_photo:\n",
        "        base += \", cartoon, anime, painting, drawing, 3d render, cgi, fake, artificial\"\n",
        "    elif is_artistic:\n",
        "        base += \", photograph, photo, realistic\"\n",
        "\n",
        "    # Quality negatives pool (select 3-5)\n",
        "    quality_pool = [\n",
        "        \"bad quality\", \"bad anatomy\", \"bad proportions\", \"deformed\",\n",
        "        \"poorly drawn\", \"ugly\", \"distorted\", \"mutation\",\n",
        "        \"disfigured\", \"malformed\", \"mutated\"\n",
        "    ]\n",
        "    base += \", \" + \", \".join(random.sample(quality_pool, k=random.randint(3, 5)))\n",
        "\n",
        "    # Portrait-specific negatives\n",
        "    if is_portrait:\n",
        "        portrait_negatives = [\n",
        "            \"bad hands\", \"poorly drawn hands\", \"bad fingers\", \"extra fingers\",\n",
        "            \"missing fingers\", \"fused fingers\", \"bad eyes\", \"asymmetric eyes\",\n",
        "            \"crossed eyes\", \"bad face\", \"poorly drawn face\", \"asymmetric face\"\n",
        "        ]\n",
        "        base += \", \" + \", \".join(random.sample(portrait_negatives, k=random.randint(4, 6)))\n",
        "\n",
        "    # Composition negatives (select 2-3)\n",
        "    composition_pool = [\n",
        "        \"cropped\", \"out of frame\", \"bad composition\", \"uncentered\",\n",
        "        \"duplicate\", \"watermark\", \"text\", \"signature\"\n",
        "    ]\n",
        "    base += \", \" + \", \".join(random.sample(composition_pool, k=random.randint(2, 3)))\n",
        "\n",
        "    # SFW-specific negatives\n",
        "    if sfw:\n",
        "        base += \", nsfw, nude, nudity, explicit, sexual content, naked\"\n",
        "\n",
        "    return base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoqSQMysiUNp"
      },
      "source": [
        "## Model Loading with Memory Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eHhc0oLiUNp"
      },
      "outputs": [],
      "source": [
        "# Check if model needs to be downloaded\n",
        "model_exists = os.path.exists(model_path) and os.path.isdir(model_path) and len(os.listdir(model_path)) > 0\n",
        "should_download = download_model or not model_exists\n",
        "\n",
        "if should_download:\n",
        "    if not model_exists:\n",
        "        print(f\"Model not found at {model_path}, downloading from HuggingFace...\")\n",
        "    elif download_model:\n",
        "        print(f\"Re-downloading model {model_id} (download_model checkbox enabled)...\")\n",
        "\n",
        "    if not huggingface_token:\n",
        "        print(\"Warning: No HuggingFace token provided. Download may fail for gated models.\")\n",
        "\n",
        "    snapshot_download(\n",
        "        repo_id=model_id,\n",
        "        local_dir=model_path,\n",
        "        token=huggingface_token if huggingface_token else None,\n",
        "        ignore_patterns=[\"*.safetensors.lock\"]\n",
        "    )\n",
        "    print(f\"✓ Model downloaded to: {model_path}\")\n",
        "else:\n",
        "    print(f\"✓ Model already exists at {model_path}, skipping download\")\n",
        "\n",
        "# Load pipeline with memory optimizations\n",
        "print(f\"Loading model {model_id}...\")\n",
        "\n",
        "# Load the correct VAE for SDXL (fixes color/quality issues)\n",
        "from diffusers import AutoencoderKL\n",
        "\n",
        "print(\"Loading optimized VAE for better color accuracy...\")\n",
        "try:\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        \"madebyollin/sdxl-vae-fp16-fix\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    print(\"✓ Loaded fp16-fixed VAE for better quality\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Could not load optimized VAE, using model default: {e}\")\n",
        "    vae = None\n",
        "\n",
        "# Base load kwargs\n",
        "load_kwargs = {\n",
        "    \"torch_dtype\": torch.float16,\n",
        "    \"use_safetensors\": True,\n",
        "}\n",
        "\n",
        "# Add VAE if successfully loaded\n",
        "if vae is not None:\n",
        "    load_kwargs[\"vae\"] = vae\n",
        "\n",
        "# Try loading with fp16 variant first, fallback to no variant if unavailable\n",
        "pipe = None\n",
        "for attempt in [\"fp16\", \"no_variant\"]:\n",
        "    try:\n",
        "        if attempt == \"fp16\":\n",
        "            load_kwargs[\"variant\"] = \"fp16\"\n",
        "            print(\"Attempting to load with fp16 variant...\")\n",
        "        else:\n",
        "            load_kwargs.pop(\"variant\", None)\n",
        "            print(\"Retrying without variant (using full precision weights)...\")\n",
        "\n",
        "        # Try loading from local path first, fall back to HuggingFace if corrupted\n",
        "        try:\n",
        "            if os.path.exists(model_path):\n",
        "                pipe = StableDiffusionXLPipeline.from_pretrained(model_path, **load_kwargs)\n",
        "                print(f\"✓ Loaded from local path: {model_path}\")\n",
        "            else:\n",
        "                raise FileNotFoundError(\"Local model path does not exist\")\n",
        "        except (OSError, FileNotFoundError) as local_error:\n",
        "            # Local model is corrupted or missing, load from HuggingFace\n",
        "            if os.path.exists(model_path):\n",
        "                print(f\"⚠ Local model corrupted: {local_error}\")\n",
        "                print(f\"Loading from HuggingFace instead...\")\n",
        "            load_kwargs[\"token\"] = huggingface_token if huggingface_token else None\n",
        "            pipe = StableDiffusionXLPipeline.from_pretrained(model_id, **load_kwargs)\n",
        "            print(f\"✓ Loaded from HuggingFace: {model_id}\")\n",
        "\n",
        "        print(f\"✓ Model loaded successfully ({attempt})\")\n",
        "        break\n",
        "\n",
        "    except ValueError as e:\n",
        "        if \"variant\" in str(e) and attempt == \"fp16\":\n",
        "            # fp16 variant not available, will retry without variant\n",
        "            continue\n",
        "        else:\n",
        "            # Some other ValueError, re-raise it\n",
        "            raise\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model with {attempt}: {e}\")\n",
        "        if attempt == \"no_variant\":\n",
        "            # Last attempt failed, re-raise\n",
        "            raise\n",
        "\n",
        "if pipe is None:\n",
        "    raise RuntimeError(\"Failed to load model after all attempts\")\n",
        "\n",
        "# Move pipeline to GPU\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# Memory optimizations\n",
        "print(\"Applying memory optimizations...\")\n",
        "pipe.enable_attention_slicing()\n",
        "pipe.enable_vae_slicing()\n",
        "pipe.enable_vae_tiling()\n",
        "\n",
        "# Try to enable xformers for better memory efficiency\n",
        "try:\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "    print(\"✓ xformers enabled\")\n",
        "except:\n",
        "    print(\"⚠ xformers not available\")\n",
        "\n",
        "# NOTE: enable_model_cpu_offload() is DISABLED because it conflicts with Compel\n",
        "# Compel needs text encoders to stay on GPU for prompt weighting\n",
        "print(\"⚠ CPU offload disabled (incompatible with Compel)\")\n",
        "\n",
        "# Initialize Compel for advanced prompt weighting\n",
        "compel = Compel(\n",
        "    tokenizer=[pipe.tokenizer, pipe.tokenizer_2],\n",
        "    text_encoder=[pipe.text_encoder, pipe.text_encoder_2],\n",
        "    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "    requires_pooled=[False, True],\n",
        "    device=\"cuda\"\n",
        ")\n",
        "print(\"✓ Compel initialized for advanced prompt weighting\")\n",
        "\n",
        "print(f\"\\n✓ Pipeline ready!\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGXveRB4iUNp"
      },
      "source": [
        "# Sampler Configuration (simplified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_7mbhnjiUNp"
      },
      "outputs": [],
      "source": [
        "# Sampler classes\n",
        "from diffusers import (\n",
        "    DPMSolverMultistepScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    KDPM2AncestralDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    UniPCMultistepScheduler\n",
        ")\n",
        "\n",
        "# Sampler factory functions - creates fresh scheduler instances\n",
        "SAMPLERS = {\n",
        "    \"DPM++ 2M\": lambda: DPMSolverMultistepScheduler(),\n",
        "    \"DPM++ 2M Karras\": lambda: DPMSolverMultistepScheduler(use_karras_sigmas=True),\n",
        "    \"Euler a\": lambda: EulerAncestralDiscreteScheduler(),\n",
        "    \"Heun\": lambda: HeunDiscreteScheduler(),\n",
        "    \"KDPM2 a\": lambda: KDPM2AncestralDiscreteScheduler(),\n",
        "    \"UniPC\": lambda: UniPCMultistepScheduler()\n",
        "}\n",
        "\n",
        "def get_resolution():\n",
        "    \"\"\"Get random resolution based on mode.\"\"\"\n",
        "    if resolution_mode == \"classic\":\n",
        "        width = random.choice(CLASSIC_RESOLUTIONS)\n",
        "        height = random.choice(CLASSIC_RESOLUTIONS)\n",
        "        return (width, height)\n",
        "    else:  # sdxl\n",
        "        return random.choice(SDXL_RESOLUTIONS)\n",
        "\n",
        "print(f\"✓ Samplers configured: {len(SAMPLERS)} available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y162nF67iUNq"
      },
      "source": [
        "# Image Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7wvOkAViUNq"
      },
      "outputs": [],
      "source": [
        "# Sampler testing removed - use settings above for generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8DPiCPGiUNq"
      },
      "source": [
        "# Generation cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqu6Jz-7iUNq"
      },
      "outputs": [],
      "source": [
        "metadata_list = []\n",
        "\n",
        "# Setup sampler with clean initialization\n",
        "print(f\"Configuring sampler: {sampler}\")\n",
        "pipe.scheduler = SAMPLERS[sampler]()\n",
        "print(f\"✓ Scheduler initialized: {pipe.scheduler.__class__.__name__}\")\n",
        "\n",
        "print(f\"\\nStarting generation of {num_of_images} images...\")\n",
        "print(f\"Save directory: {save_directory}\\n\")\n",
        "\n",
        "for i in range(num_of_images):\n",
        "    try:\n",
        "        # Get prompt from manager\n",
        "        prompt_data = prompt_manager.get_prompt()\n",
        "        if not prompt_data:\n",
        "            print(f\"Failed to get prompt for image {i}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Build final prompt with overflow\n",
        "        final_prompt, overflow_count = prompt_manager.build_weighted_prompt(\n",
        "            prompt_data, overflow_usage\n",
        "        )\n",
        "\n",
        "        # Randomize parameters\n",
        "        seed = random.randint(0, 2**32 - 1)\n",
        "        width, height = get_resolution()\n",
        "        guidance_scale = round(random.uniform(guidance_low, guidance_high) * 2) / 2\n",
        "        num_steps = random.randint(steps_low, steps_high)\n",
        "\n",
        "        # Use Compel for prompt encoding (handles long prompts)\n",
        "        conditioning, pooled = compel(final_prompt)\n",
        "\n",
        "        # Generate image\n",
        "        result = pipe(\n",
        "            prompt_embeds=conditioning,\n",
        "            pooled_prompt_embeds=pooled,\n",
        "            negative_prompt=NEGATIVE_PROMPT,\n",
        "            num_images_per_prompt=1,\n",
        "            width=width,\n",
        "            height=height,\n",
        "            guidance_scale=guidance_scale,\n",
        "            num_inference_steps=num_steps,\n",
        "            generator=torch.Generator(device=\"cuda\").manual_seed(seed),\n",
        "        ).images[0]\n",
        "\n",
        "        # Save image\n",
        "        filename = f\"{datetime.now().strftime('%Y%m%d%H%M%S')}_{str(i).zfill(3)}.png\"\n",
        "        filepath = os.path.join(save_directory, filename)\n",
        "        result.save(filepath)\n",
        "\n",
        "        # Store metadata\n",
        "        metadata = {\n",
        "            \"filename\": filename,\n",
        "            \"model\": model_id,\n",
        "            \"sampler\": sampler,\n",
        "            \"base_prompt\": base_prompt,\n",
        "            \"prompt\": prompt_data['prompt'],\n",
        "            \"final_prompt\": final_prompt,\n",
        "            \"overflow_total\": len(prompt_data.get('overflow', [])),\n",
        "            \"overflow_used\": overflow_count,\n",
        "            \"negative_prompt\": NEGATIVE_PROMPT,\n",
        "            \"sfw\": sfw,\n",
        "            \"seed\": seed,\n",
        "            \"width\": width,\n",
        "            \"height\": height,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_steps\": num_steps\n",
        "        }\n",
        "        metadata_list.append(metadata)\n",
        "\n",
        "        # Progress update\n",
        "        overflow_info = f\" +{overflow_count} overflow\" if overflow_count > 0 else \"\"\n",
        "        print(f\"[{i+1}/{num_of_images}] {width}x{height} | {num_steps} steps | G:{guidance_scale:.1f}{overflow_info}\")\n",
        "\n",
        "        # Memory cleanup\n",
        "        del result, conditioning, pooled\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image {i}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        continue\n",
        "\n",
        "# Save metadata\n",
        "metadata_path = os.path.join(save_directory, \"metadata.json\")\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata_list, f, indent=2)\n",
        "\n",
        "print(f\"\\n✓ Generation complete!\")\n",
        "print(f\"✓ {len(metadata_list)} images saved to: {save_directory}\")\n",
        "print(f\"✓ Metadata saved to: {metadata_path}\")\n",
        "\n",
        "# Print stats\n",
        "prompt_manager.print_stats()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nFinal GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0fTFpZxiUNq"
      },
      "source": [
        "## Statistics and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAd9ct_iiUNq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "if not run_sampler_test and 'metadata_list' in locals() and metadata_list:\n",
        "    # Sampler distribution\n",
        "    samplers = [m['sampler'] for m in metadata_list]\n",
        "    sampler_counts = Counter(samplers)\n",
        "\n",
        "    # Resolution distribution\n",
        "    resolutions = [f\"{m['width']}x{m['height']}\" for m in metadata_list]\n",
        "    resolution_counts = Counter(resolutions)\n",
        "\n",
        "    # Overflow usage\n",
        "    avg_overflow_used = sum(m['overflow_used'] for m in metadata_list) / len(metadata_list)\n",
        "    avg_overflow_total = sum(m['overflow_total'] for m in metadata_list) / len(metadata_list)\n",
        "\n",
        "    print(\"\\n=== Generation Statistics ===\")\n",
        "    print(f\"\\nSampler Usage:\")\n",
        "    for sampler, count in sampler_counts.most_common():\n",
        "        print(f\"  {sampler}: {count} ({count/len(metadata_list)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nTop 5 Resolutions:\")\n",
        "    for res, count in resolution_counts.most_common(5):\n",
        "        print(f\"  {res}: {count} ({count/len(metadata_list)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nOverflow Usage:\")\n",
        "    print(f\"  Average used: {avg_overflow_used:.1f}\")\n",
        "    print(f\"  Average total: {avg_overflow_total:.1f}\")\n",
        "    print(f\"  Usage rate: {avg_overflow_used/avg_overflow_total*100:.1f}%\" if avg_overflow_total > 0 else \"  N/A\")\n",
        "elif run_sampler_test:\n",
        "    print(\"ℹ️  Statistics skipped (sampler test mode - see test results above)\")\n",
        "else:\n",
        "    print(\"ℹ️  No metadata available for statistics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V9JGiQwiUNq"
      },
      "source": [
        "## Display Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vCC94IoiUNq"
      },
      "outputs": [],
      "source": [
        "#@markdown Display generated images in a grid\n",
        "show_results = True #@param {type:\"boolean\"}\n",
        "num_columns = 3 #@param {type:\"integer\"}\n",
        "max_images_to_show = 12 #@param {type:\"integer\"}\n",
        "\n",
        "if not run_sampler_test and show_results and 'metadata_list' in locals() and metadata_list:\n",
        "    display_metadata = metadata_list[:max_images_to_show]\n",
        "    num_images = len(display_metadata)\n",
        "    num_rows = (num_images + num_columns - 1) // num_columns\n",
        "\n",
        "    plt.figure(figsize=(5 * num_columns, 5 * num_rows))\n",
        "\n",
        "    for idx, metadata in enumerate(display_metadata):\n",
        "        filepath = os.path.join(save_directory, metadata['filename'])\n",
        "        img = Image.open(filepath)\n",
        "\n",
        "        plt.subplot(num_rows, num_columns, idx + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Title with key info\n",
        "        title = f\"{metadata['sampler']}\\n{metadata['width']}x{metadata['height']} | {metadata['num_steps']} steps\"\n",
        "        plt.title(title, fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Displayed {num_images} of {len(metadata_list)} images\")\n",
        "elif run_sampler_test:\n",
        "    print(\"ℹ️  Display skipped (sampler test mode - results shown above)\")\n",
        "elif not show_results:\n",
        "    print(\"ℹ️  Display disabled (set show_results = True to enable)\")\n",
        "else:\n",
        "    print(\"ℹ️  No images to display\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkQTr1ptiUNq"
      },
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLEZx4APiUNq"
      },
      "outputs": [],
      "source": [
        "#@markdown Clean up and optionally end session\n",
        "end_session = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Memory cleanup\n",
        "del pipe, compel\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Cleanup complete\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory after cleanup: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB free\")\n",
        "\n",
        "if end_session:\n",
        "    print(\"Ending Colab session...\")\n",
        "    sys.exit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}